{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "    <h1>CSCI 416-01: Introduction to Machine Learning</h1>\n",
    "    <h1>Fall 2025</h1>\n",
    "    <h1>Decision trees</h1>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "* [Decision trees](#Decision-trees)\n",
    "* [Features of decision trees](#Features-of-decision-trees)\n",
    "* [Constructing a decision tree](#Constructing-a-decision-tree)\n",
    "* [Measures of homogeneity](#Measures-of-homogeneity)\n",
    "    * [Misclassification rate](#Misclassification-rate)\n",
    "    * [Gini index](#Gini-index)\n",
    "    * [Entropy](#Entropy)\n",
    "    * [The two-class case](#The-two-class-case)\n",
    "* [Information gain](#Information-gain)\n",
    "* [Fisher's iris dataset](#Fisher's-iris-dataset)\n",
    "* [Building a decision tree in Scikit-Learn](#Building-a-decision-tree-in-Scikit-Learn)\n",
    "* [Continuous features](#Continuous-features)\n",
    "* [Missing values](#Missing-values)\n",
    "* [Overfitting](#Overfitting)\n",
    "    * [The bias-variance tradeoff](#The-bias-variance-tradeoff)\n",
    "* [Pruning](#pruning)\n",
    "* [Regression trees](#Regression-trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with scikit-learn <a id=\"scikit-learn\"></a>\n",
    "\n",
    "For most of this course we will use the Python [Scikit-Learn ML library](https://scikit-learn.org/stable/).  It can be found in [PyPl under the name <code>scikit-learn</code>](https://pypi.org/project/scikit-learn/).  It is also part of the Anaconda distribution.\n",
    "\n",
    "This notebook also requires the <code>matplotlib</code> and <code>graphviz</code> modules.  The <code>graphviz</code> module, in turn, needs you to install the [<code>DOT</code> package](https://github.com/xflr6/graphviz?tab=readme-ov-file#installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "Decision trees are easy to understand and help reveal the structure of our data.\n",
    "\n",
    "<img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/classification_tree.png\" style=\"width:500px;\"/>\n",
    "\n",
    "Some well-known examples are \n",
    "* Classification and regression trees (CART) (Breiman, Friedman, Olshen, and Stone)\n",
    "* ID3 $\\Longrightarrow$ C4.5 $\\Longrightarrow$ C5.0 (Quinlan)\n",
    "\n",
    "For now, we will look at decision trees for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features of decision trees\n",
    "\n",
    "Advantages of decision trees:\n",
    "- They are easy to explain to others.\n",
    "- They can be displayed graphically, which makes them easier to understand.\n",
    "- They easily handle qualitiative variables (e.g., weak and strong for wind).\n",
    "- They arguably reflect human decision making.\n",
    "\n",
    "Disadvantages of decision trees:\n",
    "- They arguably reflect human decision making.\n",
    "- They generally do not have the same level of predictve accuracy as other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Constructing a decision tree\n",
    "\n",
    "Variants of **Hunt's method** are commonly used to construct decision trees classifiers.\n",
    "\n",
    "Let $T$ be the set of training cases from which we will build the tree, and let $C_{1}, \\ldots, C_{K}$ be the classes.  There are three possibilities.\n",
    "1. $T$ contains one or more cases, all of which belong to a single class $C_{j}$.  The decision tree for $T$ is a leaf identifying $C_{j}$.\n",
    "2. $T$ contains no cases.  The decision tree is a leaf, but the associated class must be determined from information other than $T$ (e.g., the most frequent class at the leaf's parent).\n",
    "3. $T$ contains cases that belong to a mixture of classes.  In this case, the idea is to refine $T$ into subsets of cases that tend towards single-class collections of cases.\n",
    "    - Using a single feature we choose a test with mutually exclusive outcomes $O_{1}, \\ldots, O_{N}$, $n > 1$.\n",
    "    - Partition $T$ into subsets $T_{1}, \\ldots, T_{N}$, where $T_{i}$ contains all the cases in $T$ with outcome $O_{i}$.\n",
    "    - This yields a decision node, with a branch for each outcome.\n",
    "    - Now apply this procedure recursively to $T_{1}, \\ldots, T_{N}$.\n",
    "\n",
    "Most decision tree constructions use a greedy approach to choosing tests.\n",
    "\n",
    "Tests are selected to partition the current training set on the basis on a local measure of progress.\n",
    "  \n",
    "The goal of a test is to produce more homogeneous groups of cases $T_{1}, \\ldots, T_{N}$.\n",
    "\n",
    "To do so, we need a measure of homogeneity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of homogeneity\n",
    "\n",
    "Given a set of cases $S$, let $p_{i} = p_{i}(S)$ be the proportion of cases in $S$ that belong to class $i$.\n",
    "\n",
    "Let $m = m(S)$ be the class that makes up the majority of cases in $S$.\n",
    "\n",
    "We will classify the cases in $S$ to be instances of class $m$.\n",
    "\n",
    "Three commonly used measures of homogeneity are\n",
    "1. **Misclassification rate**: $$1 - p_{m}$$\n",
    "2. **Gini index**: $$\\sum_{\\mbox{all classes $k$}}\\!\\!\\!\\! p_{k} (1 - p_{k})$$\n",
    "3. **Entropy**: $$\\sum_{\\mbox{all classes $k$}}\\!\\!\\!\\! -p_{k} \\log_{2} p_{k}$$\n",
    "\n",
    "The entropy is sometimes scaled by 1/2 so that it has the same maximum as the other two.\n",
    "\n",
    "For all three, values closer to zero mean greater homogeneity; larger values mean greater heterogeneity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassification rate\n",
    "\n",
    "If we stopped subdividing $S$ and left it a leaf, then the logical thing to do is to assign the majority class associated with the leaf.\n",
    "\n",
    "The **misclassification rate** is the proportion of cases in $S$ incorrectly assigned to the majority class $m$.  It tells us the proportion of training cases in the leaf that are misclassified.\n",
    "\n",
    "Example: suppose there are only two outcomes for testing on an feature (as with \"Windy\").\n",
    "\n",
    "Suppose $S$ is initially a collection of 14 examples, 9 cats and 5 dogs.\n",
    "\n",
    "Then $p_{m} = 9/14$, and the misclassification rate is \n",
    "$$\n",
    "  1 - \\frac{9}{14} = \\frac{5}{14} = 0.36.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini index\n",
    "\n",
    "If instead of classifying all cases in $S$ to be the majority class, for each class $k$ we could assign them to class $k$ with probabililty $p_{k}$.\n",
    "\n",
    "The probability that you are assigned to class $k$ by mistake is then\n",
    "$$\n",
    "  p_{k} \\sum_{j \\neq k} p_{j} = p_{k} (1-p_{k}).\n",
    "$$\n",
    "Summing over all classes give the Gini index as a measure of the training error rate.\n",
    "\n",
    "In our cats/dogs example, the Gini index is\n",
    "$$ \n",
    "  \\frac{9}{14} (1 - \\frac{9}{14}) + \\frac{5}{14} (1 - \\frac{5}{14}) = \\frac{90}{196} = 0.46.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy is used in the decision tree package C5.0 and its predecessors, C4.5 and ID3.\n",
    "\n",
    "In our cats/dogs example, we will scale the entropy by 1/2 so that is more readily compared with the other measures of impurity:\n",
    "$$\n",
    "  -\\frac{1}{2} \\left(\\frac{9}{14} \\log_{2} \\frac{9}{14} + \\frac{5}{14} \\log_{2} \\frac{5}{14}\\right) = 0.47.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is entropy?\n",
    "\n",
    "To understand entropy, we need to take a detour to Shannon's theory of information.  The classic paper where these ideas are introduced is [Claude Shannon, *A mathematical theory of communication*, Bell System Technical Journal, 1948](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6773024).\n",
    "\n",
    "Shannon introduced the idea of the amount of information $I(p)$ gained by observing a random event with probability $p$.\n",
    "\n",
    "Shannon's axioms of information:\n",
    "1. Information is non-negative: $I(p) \\geq 0$.\n",
    "2. An event that always occurs provides no information: $I(1) = 0$.\n",
    "3. Information from independent events is additive: $I(pq) = I(p) + I(q)$.\n",
    "\n",
    "These axioms lead to \n",
    "$$\n",
    "  I(p) = \\log_{2} \\frac{1}{p} = -\\log_{2} p.\n",
    "$$\n",
    "Less likely events yield more information.\n",
    "\n",
    "Given a discrete random variable $X$ with possible values $p_{1}, \\ldots, p_{n}$, the **Shannon entropy** $H(X)$ is defined to be \n",
    "$$\n",
    "  H(X) = - \\sum_{i=1}^{n} p_{i} \\log_{2} p_{i}.\n",
    "$$\n",
    "The entropy is a measure of the average number of bits needed to encode a randomly drawn value of $X$, under optimal encoding (e.g., Huffman encoding).\n",
    "\n",
    "Optimal encoding uses longer codes for less likely values, using \n",
    "$\\displaystyle -\\log_{2} p_{i} = \\log_{2} 1/p_{i}$ bits for $p_{i}$.\n",
    "\n",
    "The expected number of bits to encode a value of $X$ is thus the entropy:\n",
    "$$\n",
    "  -\\sum_{i=1}^{n} p_{i} \\log_{2} p_{i}.\n",
    "$$\n",
    "\n",
    "**Shannon entropy: example.**  Consider the sequence AAAABBCD.  Let $p_{i}$ be the fraction of the time symbol $i$ appears.\n",
    "\n",
    "For this sequence, the entropy is \n",
    "\\begin{align*}\n",
    "  -\\left[ \n",
    "    \\frac{1}{2} \\log_{2} \\frac{1}{2} \n",
    "    + \\frac{1}{4} \\log_{2} \\frac{1}{4} \n",
    "    + \\frac{1}{8} \\log_{2} \\frac{1}{8} \n",
    "    + \\frac{1}{8} \\log_{2} \\frac{1}{8} \n",
    "  \\right] &= \\frac{1}{2} \n",
    "  + \\frac{2}{4} \n",
    "  + \\frac{3}{8}\n",
    "  + \\frac{3}{8} = \\frac{7}{4}.\n",
    "\\end{align*}\n",
    "This predicts that we can encode our sequence using $8 \\times \\frac{7}{4} = 14$ bits. \n",
    "\n",
    "A 16 bit encoding is \n",
    "$$\n",
    "  \\begin{array}{rclcrcl}\n",
    "    A & = & 00, & \\quad & C & = & 10 \\\\\n",
    "    B & = & 01, &       & D & = & 11,\n",
    "  \\end{array}\n",
    "$$\n",
    "so \n",
    "$$\n",
    "  \\mbox{AAAABBCD} = (00)(00)(00)(00)(01)(01)(10)(11).\n",
    "$$\n",
    "\n",
    "Huffman encoding yields a 14 bit encoding by encoding the least common symbols with the longest codes:\n",
    "$$\n",
    "  \\begin{array}{rclcrcl}\n",
    "    A & = & 0, & \\quad & C & = & 110 \\\\\n",
    "    B & = & 10, &      & D & = & 111.\n",
    "  \\end{array}\n",
    "$$\n",
    "Then \n",
    "\\begin{align*}\n",
    "  \\mbox{AAAABBCD} \n",
    "  &= \\mbox{(0)(0)(0)(0)(10)(10)(110)(111)} \\\\\n",
    "  &= \\mbox{00001010110111}.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy as a measure of (im)purity\n",
    "\n",
    "**Back to decision trees&hellip;**  In the context of developing decision node tests, the entropy tells us the average number of bits needed to encode the classification of an arbitrary member of $S$ (i.e., a member of $S$ drawn at random).\n",
    "\n",
    "If $S$ is homogeneous, we need fewer bits, so lower entropy indicates greater homogeneity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The two-class case\n",
    "\n",
    "These measures of homogeneity are most easily understood (and visualized) in the two-class case.\n",
    "\n",
    "Suppose we only have two classes, 1 and 2, in $S$.\n",
    "\n",
    "If $p$ is the proportion of one of the classes in $S$, then $(1-p)$ is the proportion of the other class.\n",
    "\n",
    "In this case we have\n",
    "<table>\n",
    "<tr><td>Misclassification rate:</td><td>$1 - \\max(p, 1-p)$</td></tr>\n",
    "<tr><td>Gini index:</td><td>$2 p (1 - p)$</td></tr>\n",
    "<tr><td>Entropy:</td><td><div style=\"width:20em;\">$-\\frac{1}{2} \\left(p \\log_{2} p + (1-p) \\log_{2} (1-p)\\right)$</div></td></tr>\n",
    "</table>\n",
    "\n",
    "Notice that all three are zero when $S$ is homogeneous ($p = 0$ or $p = 1$), and at their maximum when $S$ is evenly split ($p = 1/2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the Gini index, the entropy, and the misclassification (error) rate.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gini(p):\n",
    "    return (p)*(1 - (p)) + (1-p)*(1 - (1-p))\n",
    "\n",
    "def entropy(p):\n",
    "    return -0.5 *(p*np.log2(p) + (1 - p)*np.log2((1 - p)))\n",
    "\n",
    "def error(p):\n",
    "    return 1 - np.max([p, 1 - p])\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "ent = [entropy(p) if p != 0 else None for p in x]\n",
    "sc_ent = [e*0.5 if e else None for e in ent]\n",
    "err = [error(i) for i in x]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "for i, lab, ls, c, in zip([ent, gini(x), err], \n",
    "                          ['Entropy', 'Gini index', 'Misclassification rate'],\n",
    "                          ['-', '-', '--', '-.'],\n",
    "                          ['black', 'red', 'green', 'cyan']):\n",
    "    line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),\n",
    "          ncol=3, fancybox=True, shadow=False)\n",
    "\n",
    "ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n",
    "ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n",
    "plt.ylim([0, 0.5])\n",
    "plt.xlabel('p(i=1)')\n",
    "plt.ylabel('Impurity Index')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('./figures/impurity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information gain \n",
    "\n",
    "**Information gain** is a quantitative measure of the value of using an feature in a test, with the goal of producing more homogeneous groups as a result of testing on that feature.\n",
    "\n",
    "Henceforth, we assume that we use entropy as our measure of homogeneity.\n",
    "\n",
    "Let $A$ be an feature, $\\mbox{values}(A)$ its possible values, and $S_{v}$ be the subset of $S$ for which $A$ has value $v$.\n",
    "\n",
    "The information gain is defined to be\n",
    "$$\n",
    "  \\newcommand{\\abs}[1]{|\\, #1 \\,|}\n",
    "  \\mbox{gain}(S,A) = \\mbox{entropy}(S) - \\!\\!\\!\\!\\sum_{v \\in \\mbox{values}(S)} \\frac{\\abs{S_{v}}}{\\abs{S}} \\mbox{entropy}(S_{v}),\n",
    "$$\n",
    "where $\\abs{\\cdot}$ is the number of elements in a set.\n",
    "\n",
    "The information gain is the expected reduction in entropy caused by knowing the value of the feature $A$.\n",
    "\n",
    "**When constructing a decision node with training cases $S$, we want to choose the feature $A$ that maximizes the information gain.**\n",
    "\n",
    "In the example we have been using we have two classes: cat and dog.  There are 9 cats and 5 dogs.\n",
    "\n",
    "The initial entropy is $E = 0.470$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = entropy(9/14)  # 9 cats out of 14 animals\n",
    "print(f\"{E:.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we can now branch on one of the features <code>Height</code> and <code>Weight</code>.\n",
    "\n",
    "Suppose <code>Height</code> leads to two nodes containing:\n",
    "  1. 7 cats, 1 dog\n",
    "  2. 2 cats, 4 dogs.\n",
    "\n",
    "Let's look at the information gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropies of the two nodes resulting from a split based on height:\n",
    "p = 7/8  # 7 cats out of 8 cases\n",
    "entropy1 = entropy(p)\n",
    "print(f\"Entropy1 = {entropy1:.3}\", )\n",
    "\n",
    "p = 3/6  # 3 cats out of 6 cases\n",
    "entropy2 = entropy(p)\n",
    "print(f\"Entropy2 = {entropy2:.3}\", )\n",
    "\n",
    "# Information gain:\n",
    "gain = E - (8/14) * entropy1 - (6/14) * entropy2\n",
    "print(\"Information gain = \", gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, suppose splitting based on <code>Weight</code> leads to two nodes containing:\n",
    "  1. 3 cats, 3 dogs\n",
    "  2. 6 cats, 2 dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropies of the two nodes resulting from a split based on weight:\n",
    "p = 3/6  # 3 cats out of 6 cases\n",
    "entropy1 = entropy(p)\n",
    "print(f\"Entropy1 = {entropy1:.3}\", )\n",
    "\n",
    "p = 6/8  # 6 cats out of 8 cases\n",
    "entropy2 = entropy(p)\n",
    "print(f\"Entropy2 = {entropy2:.3}\", )\n",
    "\n",
    "# Information gain:\n",
    "gain = E - (6/14) * entropy1 - (8/14) * entropy2\n",
    "print(\"Information gain = \", gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the split on <code>Height</code> yields the greater information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher's iris dataset\n",
    "\n",
    "[Fisher's iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) is a classic data set in statistics.  It first appears in [Sir Ronald Fisher's](http://www-history.mcs.st-andrews.ac.uk/Biographies/Fisher.html) 1936 paper in which he introduces the idea of **[linear discriminant analysis (LDA)](http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/abstract;jsessionid=5A10931DC4623C414E3918FE030F64E0.f02t01)**.\n",
    "\n",
    "The data set consists of 50 samples from each of three species of iris, \n",
    "* [*I. setosa*](https://en.wikipedia.org/wiki/Iris_setosa),\n",
    "* [*I. versicolor*](https://en.wikipedia.org/wiki/Iris_versicolor)and \n",
    "* [*I. virginica*](https://en.wikipedia.org/wiki/Iris_virginica)\n",
    "  \n",
    "for a total of 150 instances.\n",
    "\n",
    "Four **features** are measured for each sample: \n",
    "1. the length of the sepal,\n",
    "2. the width of the sepal,\n",
    "3. the length of the petal, and\n",
    "4. the width of the petal.\n",
    "\n",
    "The measurements are in centimeters.\n",
    "\n",
    "The **class labels** are\n",
    "1. Iris setosa,\n",
    "2. Iris versicolor,\n",
    "3. Iris virginica.\n",
    "\n",
    "References:\n",
    "* [R. A. Fisher, *The use of multiple measurements in taxonomic problems*, Annals of Eugenics, vol. 7, no. 2, pp. 179-188, 1936.](http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/abstract;jsessionid=5A10931DC4623C414E3918FE030F64E0.f02t01)\n",
    "In this paper Fisher develops a linear discriminant model that uses a combination of these four features to distinguish the species from one other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the iris dataset from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the class labels and names.   The classes are already converted to integer labels where\n",
    "* 0 = Iris-Setosa,\n",
    "* 1 = Iris-Versicolor, and\n",
    "* 2 = Iris-Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class labels:\", np.unique(y))\n",
    "print(\"Class names:\", iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the feature data.  There is one instance per row:\n",
    "* column 1: length of sepal\n",
    "* column 2: width of sepal\n",
    "* column 3: length of petal\n",
    "* column 4: width of petal.\n",
    "\n",
    "These are all in centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.feature_names)\n",
    "print(X[0:10,:])  # Just the first 10 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a decision tree in Scikit-Learn\n",
    "\n",
    "The decision tree classifier in Scikit-Learn is called [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "A helpful feature of decision trees is that they do not require much preprocessing of the data.  In particular, there is no need for us to scale the data.\n",
    "\n",
    "We will use only the petal length and width so we will be able to plot the decision regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into 70% training and 30% test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, \n",
    "    random_state=0)  # Be sure to set the random seed so the results are reproducible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the decision regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hacked up version of https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html.\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Parameters\n",
    "num_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "\n",
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    tree,\n",
    "    X,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[2:],\n",
    "    ylabel=iris.feature_names[2:],\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(\n",
    "        X[idx, 0],\n",
    "        X[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15,\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the tree showing the training data\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The accuracy of the tree\n",
    "\n",
    "As a first pass at assessing the quality of our tree, let's look at the overall fraction of misclassified cases in both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = tree.predict(X_train)\n",
    "print('Misclassified training samples: %d' % (y_train != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test)\n",
    "print('Misclassified test samples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do better by looking for patterns of misclassification using the [**confusion matrix**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). \n",
    "\n",
    "In the confusion matrix $C = (C_{ij})$, the entry $C_{ij}$ is the number of cases in class $i$ that are predicted to be in class $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# The confusion matrix for the training data.\n",
    "y_pred = tree.predict(X_train)\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can present the confusion matrix in a fancier style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "class_names=(\"I. setosa\", \"I. versicolor\", \"I. virginica\")\n",
    "ConfusionMatrixDisplay.from_estimator(tree, X_test, y_test, display_labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the tree\n",
    "\n",
    "The resulting tree looks like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/03_18.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree was produced using the <code>dot</code> utility from the [GraphViz](http://graphviz.org) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree, \n",
    "                out_file='tree.dot', \n",
    "                feature_names=['petal length', 'petal width'])\n",
    "\n",
    "!dot -Tpng tree.dot -o tree.png\n",
    "\n",
    "Image(filename='tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous features <a id=\"continuous\"/>\n",
    "\n",
    "Continuous features can be converted to discrete features since we need a finite set of outcomes $O_{1}, \\ldots, O_{n}$.\n",
    "\n",
    "Temperature is continuous, but the value of the logical expression\n",
    "$$\n",
    "  \\mbox{(temperature $<$ 32)}\n",
    "$$\n",
    "is discrete (true or false).\n",
    "\n",
    "<table style=\"float:left;\">\n",
    "<tr><th>Forecast</th><th>Temp $(^{\\circ}F)$</th><th>Humidity</th><th>Windy?</th><th>Class</th></tr>\n",
    "<tr><td>sunny</td><td>75</td><td>70</td><td>true</td><td>Play</td></tr>\n",
    "<tr><td>sunny</td><td>80</td><td>90</td><td>true</td><td>Don't Play</td></tr>\n",
    "<tr><td>sunny</td><td>85</td><td>85</td><td>false</td><td>Don't Play</td></tr>\n",
    "<tr><td>sunny</td><td>72</td><td>95</td><td>false</td><td>Don't Play</td></tr>\n",
    "<tr><td>sunny</td><td>69</td><td>70</td><td>false</td><td>Play</td></tr>\n",
    "<tr><td>cloudy</td><td>72</td><td>90</td><td>true</td><td>Play</td></tr>\n",
    "<tr><td>cloudy</td><td>83</td><td>78</td><td>false</td><td>Play</td></tr>\n",
    "<tr><td>cloudy</td><td>64</td><td>65</td><td>true</td><td>Play</td></tr>\n",
    "<tr><td>cloudy</td><td>81</td><td>75</td><td>false</td><td>Play</td></tr>\n",
    "<tr><td>rain</td><td>71</td><td>80</td><td>true</td><td>Don't Play</td></tr>\n",
    "<tr><td>rain</td><td>65</td><td>70</td><td>true</td><td>Don't Play</td></tr>\n",
    "<tr><td>rain</td><td>75</td><td>80</td><td>false</td><td>Play</td></tr>\n",
    "<tr><td>rain</td><td>68</td><td>80</td><td>false</td><td>Play</td></tr>\n",
    "<tr><td>rain</td><td>70</td><td>96</td><td>false</td><td>Play</td></tr>\n",
    "</table>\n",
    "<img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/continuous_tree.png\" style=\"width:50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching on continuous features\n",
    "\n",
    "When devising tests on continuous features, we need to set a threshold to branch on \n",
    "(e.g., humidity $>$ 75%)\n",
    "\n",
    "**We should choose the breakpoint to maximize information gain.**\n",
    "\n",
    "If in our training data the continuous feature $A$ have the values \n",
    "$$\n",
    "  a_{1} \\leq a_{2} \\leq \\cdots a_{M},\n",
    "$$\n",
    "there are $M-1$ possible ways to split $A$ into $\\{a_{1}, \\ldots, a_{i}\\}$ and $\\{a_{i+1}, \\ldots, a_{M}\\}$.  All such splits are examined.\n",
    "\n",
    "One choice of candidate thresholds is the midpoints\n",
    "$$\n",
    "  \\frac{a_{i} + a_{i+1}}{2}.\n",
    "$$\n",
    "Alternatively, we could try the values $a_{i}$, which ensures that all threshold values come from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Geometric interpretation\n",
    "\n",
    "The treatment of continuous features in decision trees has a geometric interpretation.\n",
    "  \n",
    "Suppose we have two continuous features $x_{1}, x_{2}$ that take on values between $-2$ and $2$, and two classes, Sheep and Goat.\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/sheep_vs_goat.jpg\"/>\n",
    "</div>\n",
    "\n",
    "Suppose the decision tree is\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/sheep_goat_tree.png\" style=\"height:400px\"/>\n",
    "</div>\n",
    "The tree partitions the $(x_{1}, x_{2})$ domain: cases in the the green region are classified as Goats; cases in the yellow region are classified as Sheep.\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/tree_geometry1.png\" style=\"height:400px\"/>\n",
    "</div>\n",
    "\n",
    "If the true boundary between classes is not parallel to a coordinate axis, misclassification occurs:\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/tree_geometry2.png\" style=\"height:400px\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Missing values\n",
    "\n",
    "It may be the case that for some of the cases in our training set have missing feature values.\n",
    "\n",
    "One solution is to create a new feature value \"missing\" &ndash; we may find that cases with missing values for some feature behave differently than cases for this a value of the feature is present.\n",
    "\n",
    "CART and C4.5/C5.0 also have general approaches to missing data.\n",
    "\n",
    "In CART, whenever consider an feature for a split, only cases that have a value for that feature are considered.  CART then finds a hierarchy of **surrogate splits** using other features looking for a partition similar to the one found by excluding cases with the missing data.\n",
    "\n",
    "If a case's outcome is not known for the original test, the outcome of the first surrogate is used; if that is also unknown, the next best surrogate is used, and so on. \n",
    "\n",
    "C4.5/C5.0 use a simpler approach.  They assume that unknown test outcomes are distributed probablistically according to the relative frequency of known outcomes.  \n",
    "\n",
    "A case with an unknown test outcome is divided into fragments weighted according to these relative frequencies, so that a single case can follow multiple paths in the tree.\n",
    "\n",
    "In this approach, information gain is modified as follows.  Given an feature $A$, we compute $\\mbox{entropy}(S,A)$ and $\\mbox{entropy}(S,A_{v})$ as before, except we only use the cases where the value of $A$ is present.\n",
    "\n",
    "If $F$ is the fraction of cases for which the value of $A$ is known, then the modified information gain $\\overline{\\mbox{gain}}(S,A)$ is defined to be \n",
    "$$\n",
    "  \\overline{\\mbox{gain}}(S,A) = F \\times \\mbox{gain}(S,A).\n",
    "$$\n",
    "This gives less weight to the information gain of an feature that is missing for a lot of cases.\n",
    "\n",
    "In the initial training set $T$, all cases are given weight $w = 1$.\n",
    "\n",
    "When a case from $T$ with known outcome $O_{i}$ and weight $w$ is assigned to $T_{i}$, it is given weight $w$.\n",
    "\n",
    "Given a case with an unknown outcome, we assign it to $T_{i}$ with weight\n",
    "$$\n",
    "  w \\times \\mbox{probability of outcome $O_{i}$}.\n",
    "$$\n",
    "We estimate the probability by the relative frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "The recursive partitioning of Hunt's method will continue to subdivide the set of training cases until either \n",
    "1. each subset in the partition contains cases of a single class, or\n",
    "2. no test yields any improvement.\n",
    "\n",
    "This may result in a very complicated tree that infers more structure (overfits the training data) than is justified by the training data.\n",
    "\n",
    "**Overfitting** occurs when our model is describing random noise rather than true features, and leads to greater errors when making predictions about unseen cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of overfitting\n",
    "\n",
    "Construct a random training set.\n",
    "* There are ten features, which take on the values 0 and 1 with equal probability.\n",
    "* There are two classes; 75% have the value 0 (No) and 25% have the value 1 (Yes).\n",
    "* Split the data into a training set of the first 500 cases and a test set of the last 500 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # Make the result reproducible!\n",
    "\n",
    "X = np.greater(np.random.rand(1000,10), 0.5).astype(float)\n",
    "y = np.greater(np.random.rand(1000,1),  0.75).astype(float)\n",
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "X_train = X[:500,:]\n",
    "y_train = y[:500]\n",
    "X_test = X[500:,:]\n",
    "y_test = y[500:]\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree.predict(X_train)\n",
    "print('Accuracy on training set: %.2f' % accuracy_score(y_train, y_pred))\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "print('Accuracy on test set:     %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn builds a tree that correctly classifies 91% of the training set.\n",
    "\n",
    "However, on the test set, only 66% are correctly classified.\n",
    "\n",
    "On the other hand, a tree with a single node that classified everything as \"No\" would do better &ndash; it would be expected to correctly classify 75%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the resulting tree.  It is very complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(tree, out_file='overfit_tree.dot')\n",
    "\n",
    "!dot -Tpng overfit_tree.dot -o overfit_tree.png\n",
    "\n",
    "Image(filename='overfit_tree.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/overfit_tree.png\" style=\"height:600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What went wrong?\n",
    "\n",
    "Suppose, in a two-class problem, $p$ is the proportion of cases belonging to the majority class (\"No\", in this case).\n",
    "\n",
    "Consider two classifiers:\n",
    "1. Classifier 1 assigns all cases to the majority class; its expected error rate is $1-p$.\n",
    "2. Classifier 2 assigns a case to the majority class with probability $p$ and to the minority class with probability $1-p$; its expected error rate is the sum of\n",
    "    * the probability that a case from the majority class is assigned to the minority class: $p(1-p)$, and\n",
    "    * the probability that a case from the minority class is assigned to the majority class: $(1-p)p$.\n",
    "    \n",
    "The expected error rate for Classifier 2 is thus $2p(1-p)$.\n",
    "\n",
    "This means that if $p > 0.5$, then $2p(1-p) > 1-p$, so Classifier 2 is expected to do worse than Classifier 1.\n",
    "\n",
    "In our example with random data, the tree behaves a lot like classifier 2, sending each case randomly to leaf, which we would expect to be distributed according to the class frequencies in the training set.\n",
    "\n",
    "For the random data, $p = 0.75$, so $2p(1-p) = 2 \\times 0.75 \\times 0.25 = 0.375$, which is close to the observed error rate of 34%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complex tree was overfit &ndash; it did a great job on the training data because it went to great efforts to create small leafs.\n",
    "\n",
    "However, this structure was peculiar to the training data and did not generalize to the test data.\n",
    "\n",
    "A simpler tree actually does better.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree.predict(X_train)\n",
    "print('Accuracy on training set: %.2f' % accuracy_score(y_train, y_pred))\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "print('Accuracy on test set:     %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "export_graphviz(tree, \n",
    "                out_file='not_overfit_tree.dot')\n",
    "\n",
    "!dot -Tpng not_overfit_tree.dot -o not_overfit_tree.png\n",
    "\n",
    "Image(filename='not_overfit_tree.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the resulting tree.\n",
    "\n",
    "<img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/not_overfit_tree.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The bias-variance tradeoff\n",
    "\n",
    "This example illustrates the **bias-variance tradeoff**.\n",
    "\n",
    "**Bias** refers to the errors that occur because the model is too simple or just the wrong model for the data.\n",
    "\n",
    "**Variance** refers to the errors that reflect sensitivity of the model to the training data.  Such sensivitity may be due to overfitting.\n",
    "\n",
    "There is also likely to be an irreducible element of noise in the data that contributes to prediction errors.  Modeling the random errors leads to overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High bias: diagnosis and treatment\n",
    "The number 1 symptom of high bias is that the training error is unacceptably high.\n",
    "\n",
    "Remedies include:\n",
    "* Using a more complex model.\n",
    "    * Introducing additional features to the model.\n",
    "    * Changing the mathematical form of the model.\n",
    "* Boosting (ensemble learning).\n",
    "\n",
    "### High variance: diagnosis and treatment\n",
    "The number 1 symptom of high variance is that the training error is much lower than the test error.\n",
    "\n",
    "Remedies include\n",
    "* Adding more training data.\n",
    "* Reducing model complexity.\n",
    "* Bagging (ensemble learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pruning <a id=\"pruning\"/>\n",
    "\n",
    "In order to have predictive power and reveal problem structure, we need a significant number of cases at each leaf.  This suggests we should prefer small (short) trees.\n",
    "\n",
    "This approach is also consistent with a preference for the simplest model that fits the data (Occam's razor).\n",
    "\n",
    "There are two ways to modify the recursive tree construction to produce simpler trees.\n",
    "1. Prepruning: deciding not to partition a set of training cases any further.\n",
    "2. Postpruning: Retrospectively removing some of the subtree structure built by recursive partitioning.\n",
    "\n",
    "Stopping rules for prepruning may terminate partitioning before the benefits of subsequent partitioning becomes evident.  For this reason, CART and C4.5/C5.0 perform postpruning.\n",
    "\n",
    "Decisions trees are usually pruned by discarding one or more subtrees and replacing them with leaves that contain the cases from the subtrees.  The class associated with a new leaf is the majority class of the training cases covered by the leaf.\n",
    "\n",
    "This typically results in leaves that contain cases from more than one class, so more of the training cases will be misclassified. \n",
    "  \n",
    "On the other hand, the resulting tree tend to be more accurate on unseen cases.\n",
    "\n",
    "The presence of a distribution of classes in a leaf also leads to the perspective that we are computing probabilities that a training case belongs to that class.\n",
    "\n",
    "### Reduced error postpruning\n",
    "Suppose it were possible to predict the classification error rate of a tree and its subtrees.\n",
    "\n",
    "Then we could prune as follows:\n",
    "* Start from the bottom of the tree and examine each (nonleaf) subtree.\n",
    "* If replacing a subtree by a leaf or the subtree's most frequently used branch would lead to\n",
    "  predicted error rate that is no higher, then prune accordingly.\n",
    "     * If the predicted error rate is not reduced, but not increased, either, we still would prune in order to simplify the tree.\n",
    "\n",
    "Since the error rate of the entire tree decreases as the error rates of its subtrees are reduced, if we apply this process repeatedly then it will lead to a tree whose predicted error rate is minimal with respect to the allowable forms of pruning.\n",
    "\n",
    "### Estimating classification error in postpruning\n",
    "A simple way to estimate the classification error is to divide the set of training cases in two.\n",
    "\n",
    "One set will be used to construct the decision tree.  We then apply the tree to the other set of data to estimate errors for the purposes of pruning.\n",
    "\n",
    "The drawback is that some of the training data must be set aside and not used in constructing the tree.  If we have lots of training data, this is not a problem, but if data are scarce, it may result in an inferior tree.\n",
    "\n",
    "Other approaches are based statistical resampling (cross-validation) and heuristic corrections to the error rate observed for the training data.\n",
    "\n",
    "### Rule-based pruning\n",
    "In addition to a decision tree, C4.5/C5.0 also produce an equivalent collection of rules.\n",
    "The leftmost path in the tennis graph may be written as the rule\n",
    "<code>\n",
    "if (Outlook == Sunny) && (Humidity == High) {\n",
    "  Play_Tennis = No\n",
    "}\n",
    "</code>\n",
    "\n",
    "<img src=\"http://www.cs.wm.edu/~rml/teaching/csci416/images/tennis_tree.png\" style=\"height:300px;\"/>\n",
    "\n",
    "Rule generation leads to another approach to pruning.\n",
    "1. Build the decision tree, allowing overfitting to occur.\n",
    "2. Convert the tree into an equivalent set of rules, creating one rule for each path from the root to a leaf. \n",
    "3. Prune each rule by removing any preconditions that result in improved estimated error rate for the rule.\n",
    "4. Sort the pruned rules by their estimate accuracy, and consider them in this sorted sequence when classifiying.\n",
    "\n",
    "In the example\n",
    "<code>\n",
    "  if (Outlook == Sunny) && (Humidity == High) {\n",
    "    Play_Tennis = No\n",
    "  }\n",
    "</code>\n",
    "rule-based postpruning would consider removing (Outlook == Sunny) and (Humidity == High).\n",
    "\n",
    "Converting to a rule for each leaf allows us to distinguish between the different contexts in which a decision node is used.  We are able to prune each path individually.  \n",
    "\n",
    "By contrast, if we prune nodes, then we affect all the decision paths that are in the subtree rooted at the node. \n",
    "\n",
    "Converting to rules (arguably) makes the decision process easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression trees\n",
    "\n",
    "Decision trees can also be used for regression.  See the notes for a description of the details of constructing a regression tree.\n",
    "\n",
    "The following example is a hacked up version of\n",
    "<p>\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a random dataset.\n",
    "rng = np.random.RandomState(42)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "\n",
    "# Fit the regression model.\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "# Predict.\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "\n",
    "# Plot the results.\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Two regression trees\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The less complex tree illustrates bias.\n",
    "\n",
    "The more complex tree illustrates variance (overfitting).  It is fitting the outliers which are due in this example to random noise we added."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
