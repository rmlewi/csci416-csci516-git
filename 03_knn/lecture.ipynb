{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "    <h1>CSCI 416: Introduction to Machine Learning</h1>\n",
    "    <h1>Fall 2025</h1>\n",
    "    <h1>Nearest neighbors</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Contents\n",
    "\n",
    "- [Nearest neighbors classifiers](#Nearest-neighbors-classifiers)\n",
    "- [The choice of $k$](#The-choice-of-$k$)\n",
    "- [The choice of distance](#The-choice-of-distance)\n",
    "- [The importance of scaling](#The-importance-of-scaling)\n",
    "- [Training a kNN classifier](#Training-a-kNN-classifier)\n",
    "- [kNN and the Bayes error](#kNN-and-the-Bayes-error)\n",
    "- [Building a kNN classifier in Scikit-Learn](#Building-a-kNN-classifier-in-Scikit-Learn)\n",
    "  * [Fisher's iris data set](#Fisher's-iris-data-set)\n",
    "  * [Training and test sets](#Training-and-test-sets)\n",
    "  * [Construct the classifier](#Construct-the-classifier)\n",
    "  * [Plot the decision regions](#Plot-the-decision-regions)\n",
    "  * [Evaluate the model](#Evaluate-the-model)\n",
    "- [Feature scaling](#Feature-scaling)\n",
    "- [Pipelines in Scikit-Learn](#Pipelines-in-Scikit-Learn)\n",
    "- [Saving models](#Saving-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Nearest neighbors classifiers\n",
    "\n",
    "The $k$-nearest neighbors (kNN) classifier is very simple.\n",
    "\n",
    "Choose $k \\geq 1$.  Given  a training set $T$ and a new case $x$,\n",
    "1. Find the $k$ nearest neighbors of $x$ in $T$.\n",
    "2. Classify $x$ the be the majority class amongst the $k$ nearest neighbors.\n",
    "\n",
    "kNN is an example of a **prototype** learning method.\n",
    "\n",
    "Some methods the training data to build a mathematical model.  The mathematical model is then applied to new cases.\n",
    "\n",
    "Prototype methods, on the other hand, simply store the training data as prototypes of general cases.  When a new case is encountered, only then is the relationship between the new case and the training data explored and a decision made.\n",
    "\n",
    "Despite the simplicity of kNN, it can be a surprisingly effective classifier.  It can handles situations where the decision regions are gnarly and a class can have more than one prototype.\n",
    "\n",
    "Even though kNN is simple, there are still some hyperparameters to tune:\n",
    "- How should we choose $k$?\n",
    "- What is meant by *nearest*?  I.e., how do we measure distance or similarity/dissimilarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The choice of $k$\n",
    "\n",
    "The \"best\" choice of $k$ is highly problem-dependent.\n",
    "\n",
    "Large values of $k$ suppress the effects of noise and random variation in the data.\n",
    "However, large values of $k$ also blur the decision boundaries.\n",
    "\n",
    "Small values of $k$, on the other hand, may be more likely to fit noise in the training data.\n",
    "\n",
    "We can tune the hyperparameter $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The choice of distance\n",
    "\n",
    "Any type of norm or metric can be used in kNN.\n",
    "\n",
    "For instance, Scikit-Learn's [<code>DistanceMetric</code> class](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#distancemetric) provides a number of common metrics.\n",
    "\n",
    "We can tune the choice of distance as a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The importance of scaling\n",
    "\n",
    "When applying kNN, make sure your data are suitably scaled.\n",
    "\n",
    "For instance, suppose our data are (width, height) for boxes.  If width is measured in barleycorns while height is measured in (English) ells, then the width values will typically be 100$\\times$ that of the height values:\n",
    "$$\n",
    "  (185, 24), (167, 24.4), (195, 24.2).\n",
    "$$\n",
    "If we use the Euclidean norm to compute distance, the width values will have a much greater impact in the value of the distance between points.\n",
    "\n",
    "This is why standardizing the data (mean 0, variance 1) or scaling to a given range (e.g, [0,1]) may be a good idea.  In addition to bringing the features into a similar range, scaling also removes units and makes the data dimensionless.\n",
    "\n",
    "Otherwise, you may encounter the curious situation that your norm is combining quantities with different physical units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a kNN classifier\n",
    "\n",
    "Since kNN classifiers stores their training sets, they can be rather large.\n",
    "\n",
    "In addition, a naive implementation can be slow to apply.  Suppose \n",
    "* we are using a $p$-norm for the distance,\n",
    "* there are $n$ training cases, and\n",
    "* there are $d$ features (components) in our feature vectors.\n",
    "\n",
    "Comparing a new instance against the entire training set requires $\\Theta(nd)$ distance calculations, so making a prediction for a single instance requires $\\Theta(nd)$ work.\n",
    "\n",
    "Part of the training of a kNN classifier is the construction of special data structures for storing the training set (k-d trees or ball trees).  These data structures make the calculation of the nearest neighbor much more efficient than brute force comparision against the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN and Bayes error\n",
    "\n",
    "Assume a probabilistic relationship between the values of the input features, denoted by $x$, and the class label, denoted by $y$.  The probability that the class label is $y$ given that the input is $x$ is the conditional probability $P(y | x)$.\n",
    "\n",
    "Suppose we knew this conditional probability (in practice we do not).  Then we would simply predict the most likely label:\n",
    "$$ \\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "  y_{*} = \\argmax_{y}\\ P(y | x).\n",
    "$$\n",
    "This ideal classifier is called the **Bayes optimal classifier**.  It gives incorrect answers if a sample does not have the most likely label.  The probability of this occuring is called The **Bayes error**:\n",
    "$$\n",
    "\\newcommand{\\ebayes}{\\varepsilon_{\\mbox{\\scriptsize Bayes}}}\n",
    "  \\ebayes = 1 - P(y_{*} | x).\n",
    "$$\n",
    "It can be shown that no classifier (using the same features) can have a lower expected error rate.\n",
    "\n",
    "Cover and Hart (1967) showed that under certain mild assumptions, as the amount of training data grows to $\\infty$ the maximum error of a $1$-NN classifier error $\\varepsilon$ in binary classification converges to no more than twice the Bayes error.\n",
    "\n",
    "More precisely, the error converges to\n",
    "$$\n",
    "  \\ebayes \\leq \\varepsilon \\leq 2 \\ebayes (1-\\ebayes) \\leq 2 \\ebayes.\n",
    "$$\n",
    "This also means that with enough data we can use the $1$-NN classifier error to bound the Bayes error.\n",
    "\n",
    "Unfortunately, we are unlikely ever to have enough data. ðŸ˜¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a NN classifier in Scikit-Learn\n",
    "\n",
    "The general documentation for kNN in scikit-learn is [here](http://scikit-learn.org/stable/modules/neighbors.html).\n",
    "\n",
    "The documentation for the <code>KNeighborsClassifier</code> classifier in scikit-learn is [here](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's iris data set\n",
    "\n",
    "We will use Fisher's iris data set again.\n",
    "\n",
    "The data set consists of 50 samples from each of three species of iris, \n",
    "* [*I. setosa*](https://en.wikipedia.org/wiki/Iris_setosa),\n",
    "* [*I. versicolor*](https://en.wikipedia.org/wiki/Iris_versicolor), and \n",
    "* [*I. virginica*](https://en.wikipedia.org/wiki/Iris_virginica).\n",
    "  \n",
    "for a total of 150 instances.\n",
    "\n",
    "Four **features** are measured for each sample: \n",
    "1. the length of the sepal,\n",
    "2. the width of the sepal,\n",
    "3. the length of the petal, and\n",
    "4. the width of the petal.\n",
    "\n",
    "The measurements are in centimeters.\n",
    "\n",
    "The **class labels** are\n",
    "1. Iris setosa,\n",
    "2. Iris versicolor,\n",
    "3. Iris virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class labels:\", np.unique(y))\n",
    "print(\"Class names:\", iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.feature_names)\n",
    "print(X[0:10,:])  # Just the first 10 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the decision tree example, we will use only the petal length and width so we will be able to plot the decision regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test sets\n",
    "\n",
    "Split the data into 70% training and 30% test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y, test_size=0.3, random_state=0)  # Be sure to set the random seed so the results are reproducible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hacked up version of https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Parameters\n",
    "num_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "\n",
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    knn,\n",
    "    X,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[2:],\n",
    "    ylabel=iris.feature_names[2:],\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_train == i)\n",
    "    plt.scatter(\n",
    "        X_train[idx, 0],\n",
    "        X_train[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15,\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the tree showing the training data\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that kNN can produce complex shapes in its decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = knn.predict(X_train)\n",
    "print('Misclassified training samples: %d' % (y_train != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print('Misclassified test samples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# The confusion matrix for the training data.\n",
    "y_pred = knn.predict(X_train)\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "class_names=(\"I. setosa\", \"I. versicolor\", \"I. virginica\")\n",
    "ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test, display_labels=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    knn,\n",
    "    X,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[2:],\n",
    "    ylabel=iris.feature_names[2:],\n",
    ")\n",
    "\n",
    "# Plot the test points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_test == i)\n",
    "    plt.scatter(\n",
    "        X_test[idx, 0],\n",
    "        X_test[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15,\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the tree showing the test data\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "\n",
    "The relative scaling of the data affects the performance of many training algorithms as well as the performance of the resulting ML model.\n",
    "\n",
    "The iris data is already pretty well scaled: across all values there is only a spread of two orders of magnitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean:      {np.mean(X, axis=0)}\")\n",
    "print(f\"std. dev.: {np.std(X, axis=0)}\")\n",
    "\n",
    "q = np.quantile(X, (0, 0.25, 0.5, 0.75, 1), axis=0)\n",
    "iqr = q[3] - q[1]\n",
    "print(f\"median:    {np.median(X, axis=0)}\")\n",
    "print(f\"IQR:       {iqr}\")\n",
    "print(f\"quantiles:\")\n",
    "print(f\"{q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rescale the iris training and test data to have mean 0 and variance 1 using [<code>StandardScaler</code>](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#standardscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)  # Compute the mean and std to be used for later scaling.\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std  = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "knn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = knn.predict(X_train_std)\n",
    "print('Misclassified training samples: %d' % (y_train != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test_std)\n",
    "print('Misclassified test samples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "class_names=(\"I. setosa\", \"I. versicolor\", \"I. virginica\")\n",
    "ConfusionMatrixDisplay.from_estimator(knn, X_test_std, y_test, display_labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the decision regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "\n",
    "X_std = sc.transform(X)\n",
    "\n",
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    knn,\n",
    "    X_std,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[2:],\n",
    "    ylabel=iris.feature_names[2:],\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_train == i)\n",
    "    plt.scatter(\n",
    "        X_train_std[idx, 0],\n",
    "        X_train_std[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15,\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the tree showing the training data\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    knn,\n",
    "    X_std,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[2:],\n",
    "    ylabel=iris.feature_names[2:],\n",
    ")\n",
    "\n",
    "# Plot the test points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_test == i)\n",
    "    plt.scatter(\n",
    "        X_test_std[idx, 0],\n",
    "        X_test_std[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15,\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the tree showing the test data\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines in Scikit-Learn\n",
    "\n",
    "If we add scaling to our process we will need to make sure that the scaling is applied before the model is applied.  Rather than require the user to understand and implement all necessary preprocessing, Scikit-Learn makes it simple to package a whole stream of computation using [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).  Per the documentation:\n",
    "<blockquote>\n",
    "Pipeline allows you to sequentially apply a list of transformers to preprocess the data and, if desired, conclude the sequence with a final predictor for predictive modeling.\n",
    "\n",
    "Intermediate steps of the pipeline must be â€˜transformsâ€™, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "</blockquote>\n",
    "\n",
    "Let's roll up the scaling and kNN classifier we just did into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "sc = StandardScaler()\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "\n",
    "clf = make_pipeline(sc, knn)\n",
    "\n",
    "print(clf)\n",
    "print(clf[0])\n",
    "print(clf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm the pipeline behaves like what we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Misclassified training samples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    knn,\n",
    "    X_std,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[2:],\n",
    "    ylabel=iris.feature_names[2:],\n",
    ")\n",
    "\n",
    "# Plot the test points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_test == i)\n",
    "    plt.scatter(\n",
    "        X_test_std[idx, 0],\n",
    "        X_test_std[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15,\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the tree showing the training data\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving models\n",
    "\n",
    "The simplest way to save a Scikit-Learn model is to use the Python built-in [pickle](https://docs.python.org/3/library/pickle.html) module.\n",
    "\n",
    "Other solutions are discussed in the [Scikit-Learn User Guide](https://scikit-learn.org/stable/model_persistence.html#model-persistence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    dump(clf, f)\n",
    "\n",
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    model = load(f)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
