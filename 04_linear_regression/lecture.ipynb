{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "    <h1>CSCI 416-01/516-01: Fundamentals of AI and ML, Fall 2025</h1>\n",
    "    <h1>Linear least-squares and regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents \n",
    "\n",
    "- [Fitting a line](#Fitting-a-line)\n",
    "- [Linear models and least squares](#Linear-models-and-least-squares)\n",
    "- [The normal equations](#The-normal-equations)\n",
    "- [The California housing dataset](#The-California-housing-dataset)\n",
    "    - [Visualizing characteristics of our dataset](#Visualizing-characteristics-of-our-dataset)\n",
    "- [Building a least squares model](#Building-a-least-squares-model)\n",
    "     - [The normal equations](#The-normal-equations)\n",
    "- [Training and test sets](#Training-and-test-sets)\n",
    "- [Regularization](#Regularization)\n",
    "    - [Ridge regression](#Ridge-regression)\n",
    "    - [LASSO](#LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://imgs.xkcd.com/comics/curve_fitting.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a line\n",
    "\n",
    "The simplest example of regression is fitting a line to a set of data.  Think of the $x$ coordinate as the predictive variable and the $y$ coordinate as the observation.\n",
    "\n",
    "In this case the model for the data (green squares) is a line:\n",
    "$$\n",
    "y(x) = mx + b.\n",
    "$$\n",
    "\n",
    "But which line should we choose?  We will choose the $w_{0}, w_{1}$ that minimize a measure of the misfit between observations and model predictions.\n",
    "\n",
    "In classical least squares, if the data are $(x_{1}, y_{1}), \\ldots, (x_{t}, y_{t})$, we want to choose $m, b$ that minimize\n",
    "$$\n",
    "\\sum_{i=1}^{t} (y_{i} - (mx_{i} + b))^{2}.\n",
    "$$\n",
    "The name \"least squares\" refers to the fact that we are finding the least value of this sum of squares measure of misfit.\n",
    "\n",
    "Other measures of misfit could be used, e.g.,\n",
    "$$\n",
    "\\sum_{i=1}^{t} \\lvert y_{i} - (bx_{i} + m) \\rvert,\n",
    "$$\n",
    "but the least squares misfit turns out to yield a particularly tractable optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models and least squares\n",
    "\n",
    "More generally, suppose we have $x = (x_{1}, \\ldots, x_{t}) \\in \\mathbb{R}^{n}$ and associated values $y_{1}, \\ldots, y_{t}$ (we'll assume the $y$ are scalars for simplicity).\n",
    "\n",
    "Given functions $\\phi_{1}(x), \\ldots, \\phi_{n}(x)$, a model of the form\n",
    "$$\n",
    "  w_{1} \\phi_{1}(x) + w_{2} \\phi_{2}(x) + \\cdots + w_{n} \\phi_{n}(x)\n",
    "$$\n",
    "is called **linear model** because the parameters $w_{i}$ enter the problem linearly.  The model itself may be a nonlinear function of $x$.\n",
    "\n",
    "Ideally we would have\n",
    "\\begin{align*}\n",
    "  w_{1} \\phi_{1}(x_{1}) + w_{2} \\phi_{2}(x_{1}) + \\cdots + w_{n} \\phi_{n}(x_{1}) &= y_{1} \\\\\n",
    "  w_{1} \\phi_{1}(x_{2}) + w_{2} \\phi_{2}(x_{2}) + \\cdots + w_{n} \\phi_{n}(x_{2}) &= y_{2} \\\\\n",
    "  \\vdots &= \\vdots \\\\\n",
    "  w_{1} \\phi_{1}(x_{t}) + w_{2} \\phi_{2}(x_{t}) + \\cdots + w_{n} \\phi_{n}(x_{t}) &= y_{t}. \n",
    "\\end{align*}\n",
    "This is a system of linear equations in the $w_{i}$.  If we let $A$ be the $t \\times n$ matrix whose $(i,j)$ entry is $\\phi_{j}(x_{i})$, $w = (w_{1}, \\ldots, w_{n})$, and $y = (y_{1}, \\ldots, y_{t})$, we can express this linear system as\n",
    "$$\n",
    "Aw = y.\n",
    "$$\n",
    "However, if $t > n$ this system may fail to have a solution (there are more equations than unknowns).  On the other hand, if $t < n$ this system still might not have a solution but if it has one, there are infinitely many solutions (more unknowns than equations).\n",
    "\n",
    "We address these problems by relaxing the requirement that $Aw = y$.  Instead, we require $w$ to minimize \n",
    "$$\n",
    "\\sum_{i=1}^{t} (Aw - y)_{i}^{2} = \\| Aw - y \\|_{2}^{2}.\n",
    "$$\n",
    "The quantity $Aw - y$ is called the **residual**.  A minimizer of this function in $w$ is called a **least squares solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The normal equations\n",
    "\n",
    "Minimizers of the least squares function have a simple linear algebraic characterization.\n",
    "\n",
    "**Theorem.**  A vector $w$ is a least squares solution of $Aw = y$ if and only if $A^{T}A w = A^{T} y$.\n",
    "\n",
    "The linear system $A^{T}A w = A^{T} y$ is called the **normal equations**.\n",
    "\n",
    "The name \"normal equations\" comes from the following terminology: a real matrix $N$ is called **normal** if $N^{T}N = NN^{T}$.  In the case of the normal equations the matrix $A^{T}A$ is normal.\n",
    "\n",
    "**Proof.**  First we show that if $w$ is a least squares solution then it satisfies the normal equations.  Suppose $w$ minimizes \n",
    "$F(w) = \\| Aw - y \\|_{2}^{2}$.  Then, $F(w) \\leq F(u)$ for all other $u$.  Choose an arbitrary vector $h$ and define the following function of a single real variable $t$:\n",
    "\\begin{align*}\n",
    "  \\phi(t)\n",
    "  &= F(w+th) = (w+th)^{T} A^{T}A (w+th) - 2(w+th)^{T}A^{T}y + y^{T}y \\\\\n",
    "  &= w^{T} A^{T}A w - 2w^{T}A^{T}y + y^{T}y + (h^{T} A^{T}A h) t^{2} + 2(h^{T}A^{T}(Aw - y))t \\\\\n",
    "  &= F(w) + (h^{T} A^{T}A h) t^{2} + 2(h^{T}A^{T}(Aw - y))t.\n",
    "\\end{align*}\n",
    "Since $\\phi(0) = F(w)$, we know that $\\phi(0) \\leq \\phi(t)$ for all other $t$.  Since $0$ is a local minimizer of $\\phi$, it follows that $\\phi'(0) = 0$.  Since\n",
    "\\begin{align*}\n",
    "  \\phi'(t) &= 2(h^{T} A^{T}A h) t + 2(h^{T}A^{T}(Aw - y)) \\\\\n",
    "  \\phi'(0) &= 2(h^{T} A^{T} (Aw - y),\n",
    "\\end{align*}\n",
    "we must have\n",
    "$$\n",
    "  h^{T}A^{T}(Aw - y) = 0.\n",
    "$$\n",
    "However, $h$ was arbitrary, so this equation must hold for all $h$, so we must have\n",
    "$$\n",
    "  A^{T} (Aw - y) = 0,\n",
    "$$\n",
    "which is what we wished to show.\n",
    "\n",
    "Conversely, suppose $y$ satisfies the normal equation: $A^{T}(Aw - y) = 0$.  Let $z$ be another point in $\\mathbb{R}^{n}$.  Set $h = z - w$ and as above define \n",
    "$$\n",
    "  \\phi(t) = F(w+th) = F(w) + (h^{T} A^{T}A h) t^{2} + 2(h^{T}A^{T}(Aw - y)t.\n",
    "$$\n",
    "Since $A^{T}(Aw - y) = 0$, the last term on the right vanishes, leaving us with\n",
    "$$\n",
    "  \\phi(t) = F(w+th) \n",
    "    = F(w) + (h^{T} A^{T}A h) t^{2} \n",
    "    = F(w) + \\|A h \\|_{2}^{2} t^{2} \n",
    "    \\geq F(w).\n",
    "$$\n",
    "Letting $t = 1$ we obtain\n",
    "$$\n",
    "  \\phi(1) = F(w+(z-w)) = F(z) \\geq F(w).\n",
    "$$\n",
    "Since $z$ was arbitrary, we see that $F(w) \\leq F(z)$ for all $z$, so $w$ is a least squares solution. 👍\n",
    "\n",
    "An attraction of the normal equations is that if $A^{T}A$ is nonsingular we can solve\n",
    "$$\n",
    "  A^{T}A w = A^{T}y\n",
    "$$\n",
    "using the $LU$ decomposition (Gaussian elimination) or the Cholesky factorization.  Moreover, if $A \\in \\mathbb{R}^{m \\times n}$, then $A^{T}A \\in \\mathbb{R}^{n \\times n}$.  Since $n$ is the number of terms in our linear model, it is likely not very large and we have a modest size of linear system to solve.\n",
    "\n",
    "If $A^{T}A$ is ill-conditioned or singular we can use the $QR$ decomposition.  Since we will need to apply $QR$ to the $m \\times n$ matrix $A$, it is likely to be considerably more expensive than the normal equation.  On the other hand, $QR$ is numerically extremely stable.\n",
    "\n",
    "Alternatively, if $A^{T}A$ is ill-conditioned or singular we can apply regularization (see below) or the pseudoinverse.  The pseudoinverse $A^{+}$ will, in the case where $A$ does not have full row rank and least squares solutions are not unique, will yield the least squares solution with the smallest 2-norm.  The pseudoinverse is typically computed from the singular value decomposition (SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The California housing dataset\n",
    "\n",
    "We will build a regression model to predict housing prices in California.  Our dataset has 8 features (described below) and a single target of price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "ca = fetch_california_housing()\n",
    "print(ca.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing characteristics of our dataset\n",
    "\n",
    "The Python package [seaborn](http://seaborn.pydata.org/) is a statistical visualization package built on top of [matplotlib](https://matplotlib.org).  You will need to install the <code>seaborn</code> module.\n",
    "\n",
    "First we convert our feature data to a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = ca.feature_names\n",
    "df = pd.DataFrame(ca.data, columns=cols)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like these are decimal degrees for latitude and longitude.  Since California is in the Western hemisphere the longitude will become more negative as we travel west."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ca.target, columns=[\"MedPrice\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise plots\n",
    "\n",
    "A simple way to gain insight into multivariate data is to plot pairs of the variables.  The seaborn <tt>pairplot()</tt> function does this.\n",
    "\n",
    "The pairwise plot of a variable with itself is a histogram of that variable's values.\n",
    "\n",
    "This may take a little time to generate the 64 plots&hellip;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "sns.pairplot(df, height=2.5)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/scatter.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise correlations\n",
    "\n",
    "We can also look at the pairwise correlations of variables.\n",
    "\n",
    "This plot uses Pearson's correlation coefficient $r$.  We have $-1 \\leq r \\leq 1$.  If $r < 0$ the variables are negatively correlated&ndash;one tends to decrease as the other increases.  If $r > 0$ the variables are positively correlated&ndash;one tends to increase as the other increases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cm = np.corrcoef(df.values.T)\n",
    "sns.set(font_scale=1)\n",
    "sns.set(rc={\"figure.figsize\":(8, 8)})\n",
    "hm = sns.heatmap(cm, \n",
    "            cbar=True,\n",
    "            annot=True, \n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            annot_kws={'size': 15},\n",
    "            yticklabels=cols,\n",
    "            xticklabels=cols)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/corr_mat.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a least squares model\n",
    "\n",
    "We want find a predictor for y, the median house price in each area.  The model will be a weighted combination of the features in our data:\n",
    "$$\n",
    "w_{0} + w_{1} x_{1} + \\cdots + w_{n} x_{n}.\n",
    "$$\n",
    "We seek model parameters $w$ that minimize\n",
    "$$\n",
    "\\| Aw - y_\\textrm{train} \\|_{2}^{2},\n",
    "$$\n",
    "where $A$ is the training data <code>X_train</code> with a column of ones prepended:\n",
    "\n",
    "$$\n",
    "  Aw = \\begin{pmatrix}\n",
    "         1 & x_{1}^{(1)} & \\cdots & x_{n}^{(1)} \\\\\n",
    "         1 & x_{1}^{(2)} & \\cdots & x_{n}^{(2)} \\\\\n",
    "         \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "         1 & x_{1}^{(t)} & \\cdots & x_{n}^{(t)}\n",
    "       \\end{pmatrix}\n",
    "       \\begin{pmatrix}\n",
    "         w_{0} \\\\ w_{1} \\\\ \\vdots \\\\ w_{n}\n",
    "       \\end{pmatrix}\n",
    "     = \\begin{pmatrix}\n",
    "         w_{0} + x_{1}^{(1)}w_{1} + \\cdots + x_{n}^{(1)}w_{n} \\\\\n",
    "         w_{0} + x_{1}^{(2)}w_{1} + \\cdots + x_{n}^{(2)}w_{n} \\\\\n",
    "         \\vdots \\\\\n",
    "         w_{0} + x_{1}^{(t)}w_{1} + \\cdots + x_{n}^{(t)}w_{n}\n",
    "       \\end{pmatrix}.\n",
    "$$      \n",
    "\n",
    "To make a prediction, we take the test inputs <code>X_test</code>, prepend a column of ones to form a new matrix $\\hat{A}$, and then compute $\\hat{A} w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our typical 70/30 split for training/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ca.data\n",
    "y = ca.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y.reshape(-1,1), test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The normal equations\n",
    "\n",
    "Let's look at the normal operator $A^{T}A$ for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_train.T @ X_train\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while $A$ is $20640 \\times 8$, $A^{T}A$ is only $8 \\times 8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"2-norm condition number: {np.linalg.cond(N, p=2):.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number is acceptable if we work in <code>binary64</code> floating-point.  \n",
    "\n",
    "It is totally unacceptable if we work in <code>binary32</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data  \n",
    "\n",
    "Let's apply the standard scaling (mean 0, variance 1) to the inputs and see the effect on the normal operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "X_train_std = sc_x.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the condition number for the normal operator for the scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_train_std.T @ X_train_std\n",
    "print(N)\n",
    "print(N, end=\"\\n\\n\")\n",
    "print(f\"2-norm condition number: {np.linalg.cond(N, p=2):.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much better condition number!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and fit the linear regressor\n",
    "\n",
    "We use the [LinearRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "use_scaled_data = False\n",
    "price_scaling = 1\n",
    "\n",
    "slr = LinearRegression(fit_intercept=False)\n",
    "if use_scaled_data:\n",
    "    sc = StandardScaler()\n",
    "    clf = make_pipeline(sc, slr)\n",
    "else:\n",
    "    clf = make_pipeline(slr)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "\n",
    "We will transform the predictions back to the original variables since its easier to understand the units ($100,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the performance of our predictor\n",
    "\n",
    "Let's look at the errors in our predictions.  Remember that our predictions are in units of $100,000, so we will scale the residual so that we are seeing the values in dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Compute the residual: the vector of mismatches between prediction and truth.\n",
    "r = y_pred - y_test\n",
    "relerr = r / y_test  # Relative error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.column_stack((price_scaling * r, price_scaling * np.abs(r), relerr, np.abs(relerr)))\n",
    "pd.DataFrame(E, columns=(\"Error\", \"Absolute error\", \"Relative error\", \"Absolute relative error\")).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the regression model\n",
    "\n",
    "The model parameters are in the attributes <code>coef_</code> and <code>intercept_</code>.  The intercept term is the constant term in our model.  It gives the expected value for the output when all of the predictor variables are zero if this situation is meaningful (which it is not, for this model).\n",
    "\n",
    "The relative magnitudes of the coefficients (taking into account the rescaling of the variables) reflects which variables matter most in making the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(slr, \"intercept\"):\n",
    "    print(f\"Intercept term: {slr.intercept_[0]:e}\")\n",
    "for coeff, var in zip(slr.coef_[0], ca.feature_names):\n",
    "    print(f\"{coeff:+5.3f} * {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the unscaled data we see that the most important predictors are median income (MedInc) and the average number of bedrooms.  Interesting, as the average number of rooms increases, the model predicts the price to decrease.\n",
    "\n",
    "As the latitude and longitude increase, the model also predicts the price to decrease.  Can you think of why this might be so?\n",
    "\n",
    "When looking at the coefficients of our linear model to try to understand the influence of each explanatory variable, you need to keep in mind\n",
    "* the magnitude of the coefficients,\n",
    "* the magnitude of the associated explanatory variables,\n",
    "* the effect of scaling.\n",
    "\n",
    "In addition, many of the features are themselves correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check that the normal equations are satisfied for our model parameters $w$ and the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array(slr.coef_[0])\n",
    "AtAw = X_train.T @ (X_train @ w)\n",
    "AtAw.shape = (8,1)\n",
    "Aty = X_train.T @ y_train\n",
    "print(AtAw - Aty)\n",
    "\n",
    "relerr = np.linalg.norm(AtAw - Aty, ord=2)/np.linalg.norm(Aty, ord=2)\n",
    "print(f\"Relative error (2-norm): {relerr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we can also solve least squares problems via the SVD and the pseudoinverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_plus = np.linalg.pinv(X_train)\n",
    "print(A_plus @ y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "We can also control overfitting in regression by using **regularization**.  We add a regularization term to the function to the least-squares error function that penalizes large values of the model coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "Ridge regression is a regularized variant of ordinary least squares. It computes $w$ by minimizing\n",
    "$$\n",
    "\\|\\; Aw - y \\;\\|_{2}^{2} + \\alpha \\|\\; w \\;\\|_{2}^{2},\n",
    "$$\n",
    "where $\\alpha > 0$.  The 2-norm penalty term keeps the model parameters from becoming large.  The larger the value of $\\alpha$, the greater the effect of the penalty term.\n",
    "\n",
    "The solution to this problem satisfies\n",
    "$$\n",
    "  (A^{T}A + \\alpha I) w = A^{T} y.\n",
    "$$\n",
    "Even if $A^{T}A$ is not invertible the modified matrix $A^{T}A + \\alpha I$ will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=10)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ridge.predict(X_test)\n",
    "print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = y_pred - np.squeeze(y_test)\n",
    "relerr = r / np.squeeze(y_test)\n",
    "print(r.shape)\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "E = np.column_stack((price_scaling * r, price_scaling * np.abs(r), relerr, np.abs(relerr)))\n",
    "print(E.shape)\n",
    "pd.DataFrame(E, columns=(\"Error\", \"Absolute error\", \"Relative error\", \"Absolute relative error\")).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO\n",
    "\n",
    "LASSO (least absolute shrinkage and selection operator) is a variant of ordinary least squares. It computes $w$ by minimizing\n",
    "$$\n",
    "\\|\\; Aw - y \\;\\|_{2}^{2} + \\alpha \\|\\; w \\;\\|_{1}.\n",
    "$$\n",
    "Again, the penalty term keeps the model parameters from becoming large.  However, this function is not differentiable everywhere and there is no simple characterization of its minimizer.\n",
    "\n",
    "The use of the 1-norm (rather than the 2-norm) has the effect of making components of $w$ zero.  This means that some of the input variables are ignored, thereby simplifying the model.  Higher values of the penalty weight $\\alpha$ increases the number of variables that are ignored, at the possible cost of increasing the training and prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that some of the predictor coefficients are zero.  This means they do not figure in the predictor, which means our predictor is simpler than the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso.predict(X_test)\n",
    "y_pred.shape = (6192, 1)\n",
    "\n",
    "r = y_pred - y_test\n",
    "relerr = r / y_test\n",
    "print(y_pred.shape, y_test.shape, relerr.shape)\n",
    "\n",
    "E = np.column_stack((price_scaling * r, price_scaling * np.abs(r), relerr, np.abs(relerr)))\n",
    "pd.DataFrame(E, columns=(\"Error\", \"Absolute error\", \"Relative error\", \"Absolute relative error\")).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once $\\alpha$ is sufficiently large all the variables are ignored and the solution is $w = 0$.  This behavior is related to the exact penalization technique in optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lasso.predict(X_test)\n",
    "print(lasso.coef_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
