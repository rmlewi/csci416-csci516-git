{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "</div>\n",
    "\n",
    "<h1 style=\"text-align:center;\">CSCI 416-01/CSCI 516-01: Fundamentals of AI/ML, Fall 2025</h1>\n",
    "<h1 style=\"text-align:center;\">Linear classifiers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "* [Linear classifiers](#Linear-classifiers)\n",
    "* [Bayes theorem](#Bayes-theorem)\n",
    "* [Discriminant analysis](#Discriminant-analysis)\n",
    "  * [Linear discriminant analysis](#Linear-discriminant-analysis)\n",
    "    * [Training an LDA model](#Training-an-LDA-model)\n",
    "  * [Quadratic discriminant analysis](#Quadratic-discriminant-analysis)\n",
    "* [Logistic regression](#Logistic-regression)\n",
    "  * [Binary logistic regression](#Binary-logistic-regression)\n",
    "  * [Maximum likelihood estimation](#Maximum-likelihood-estimation)\n",
    "* [Fisher's iris data set](#Fisher's-iris-data-set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Linear classifiers\n",
    "\n",
    "$$\n",
    "  \\newcommand{\\cprob}[2]{P(#1 \\,|\\, #2)}\n",
    "  \\newcommand{\\R}{\\mathbb{R}}\n",
    "  \\newcommand{\\Rn}{\\R^{n}}\n",
    "$$\n",
    "\n",
    "In **linear classification** the decision boundaries are hyperplanes.\n",
    "\n",
    "In $n$ dimensions, a hyperplane has the form\n",
    "$$\n",
    "  \\{x \\in \\Rn | w^{T}x = w_{1}x_{1} + \\cdots + w_{n}x_{n} = b\\}\n",
    "$$\n",
    "for some $w \\in \\Rn$ and $b \\in \\R$.\n",
    "\n",
    "In $\\R^{2}$ a hyperplane is a line, and in $\\R^{3}$ it is a plane.\n",
    "\n",
    "Examples of linear classifiers:\n",
    "* linear discriminant analysis (LDA),\n",
    "* logistic regression,\n",
    "* support vector machines (SVM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes theorem\n",
    "\n",
    "**Bayes Theorem.**  Let $E$ and $A$ be two events in a probability space.  Then\n",
    "$$\n",
    "  \\cprob{E}{A} = \\frac{\\cprob{A}{E}\\; P(E)}{P(A)}\n",
    "$$\n",
    "\n",
    "A spam filter is a two-class classifier: mail is either spam (bad) or ham (good).\n",
    "\n",
    "Let $E$ denote that an email is spam, while $A$ is, say, the presence of the word ``lottery'' in an email.\n",
    "\n",
    "For the purposes of Bayes theorem,\n",
    "* $\\cprob{A}{E}$ is called the **likelihood**. It is the probability that the feature $A$ appears given that $E$ is spam.\n",
    "* $P(E)$ is the probability that an email is spam.  In this context it is called the **prior**, as it is what we know prior ro knowing that feature $A$ is present in an email.\n",
    "* The quantity $P(A)$ is the probability that $A$ appears.  It normalizes the right-hand side so that the quotient is between $0$ and $1$.\n",
    "\n",
    "We are interested in $\\cprob{E}{A}$, the **posterior distribution**.  It is the probability of $E$ (spam) after taking the relevant information that $A$ is present into account.  We adjust our prior probability $P(E)$ in light of the presence of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Suppose there is a disease that strikes 1 person in 100,000.  We have a test for the disease:\n",
    "* If you have the disease, you test positive 99% of the time.\n",
    "* If you do not have the disease, you incorrectly test positive 1% of the time.\n",
    "\n",
    "Question: if you test positive, what is the probability you have the disease?\n",
    "\n",
    "Let\n",
    "* P(sick | positive) be the probability you are sick if you test positive;\n",
    "* P(positive) be the probability you test positive;\n",
    "* P(positive | sick) and P(positive | not sick) be the probabilities you test positive if you are sick or are not sick;\n",
    "* P(sick) and P(not sick) be the probabilities you are or are not sick;\n",
    "\n",
    "The positive results consist of those who are sick and test positive, and those who are not sick and test\n",
    "positive:\n",
    "$$\n",
    "  \\mbox{P(positive)} = \\mbox{P(positive | sick) $\\times$ P(sick) + P(positive | not sick) $\\times$ P(not sick)}.\n",
    "$$\n",
    "Bayes theorem says\n",
    "$$\n",
    "  \\mbox{P(sick | positive)}\n",
    "  = \\frac{\\mbox{P(positive | sick) $\\times$ P(sick)}}{\\mbox{P(positive)}}\n",
    "  = \\frac{0.99 \\times 0.00001}{0.99 \\times 0.00001 + 0.01 \\times 0.99999}\n",
    "  \\approx 0.001,\n",
    "$$\n",
    "or one chance in 1,000---even if you test positive it is not very likely you have the disease.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A generalization of Bayes theorem\n",
    "\n",
    "$$\n",
    "  \\newcommand{\\cprob}[2]{P(#1 \\,|\\, #2)}\n",
    "$$\n",
    "  \n",
    "Let $E_{1}, \\ldots, E_{n}$ be mutually exclusive events that partition the probability space $S$.  Then\n",
    "$$\n",
    "\\cprob{E_{i}}{A} = \\frac{\\cprob{A}{E_{i}} P(E_{i})}{\\sum_{k=1}^{n} \\cprob{A}{E_{k}} P(E_{k})}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminant analysis\n",
    "\n",
    "In the classification setting, let $E_{i}$ be the event $x \\in E_{i}$.  \n",
    "\n",
    "The generalized Bayes theorem tells us that\n",
    "$$\n",
    "  \\cprob{E_{i}}{X = x} = \\frac{\\cprob{X = x}{E_{i}} P(E_{i})}{\\sum_{k=1}^{n} \\cprob{X = x}{E_{k}} P(E_{k})}.\n",
    "$$\n",
    "The term $\\cprob{E_{i}}{X = x}$ is the conditional probability that the class is $i$ given the input $x$.\n",
    "\n",
    "The $\\cprob{X = x}{E_{k}}$ terms are the class conditional probability densities of $X$ in class $k$.\n",
    "\n",
    "We know that the Bayes optimal classifier would be to assign $x$ to the class $k$ for which $\\cprob{E_{k}}{X = x}$ is largest.  From the preceding equation we see that this is the same as choosing the class with the largest value of\n",
    "$$\n",
    "  \\cprob{X = x}{E_{i}} P(E_{i}).\n",
    "$$\n",
    "\n",
    "In linear and quadratic discriminant analysis we assume a particular statistical structure for our data.  In both cases we assume that the class densities are multivariate Gaussians:\n",
    "$$\n",
    "\\newcommand{\\abs}[1]{| #1 |}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\cprob{X = x}{E_{k}} = \\frac{1}{(2\\pi)^{p/2} \\abs{\\Sigma_{k}}^{1/2}}\n",
    "e^{-\\half (x - \\mu_{k})^{T} \\Sigma_{k}^{-1} (x - \\mu_{k})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant analysis\n",
    "\n",
    "In linear discriminant analysis (LDA) we assume all of our classes have the same covariance matrix: $\\Sigma_{k} = \\Sigma$ for all $k$.\n",
    "\n",
    "Under this assumption it turns out that choosing the class with the largest value of $\\cprob{X = x}{E_{i}} P(E_{i})$ is the same as choosing the class with the largest value of \n",
    "$$\n",
    "  d_{i}(x) = x^{T} \\Sigma^{-1} \\mu_{i} - \\half \\mu_{i}^{T} \\Sigma^{-1} \\mu_{i} + \\log P(E_{i}).\n",
    "$$\n",
    "This decision function is linear in $x$.\n",
    "\n",
    "The decision boundary between classes $i$ and $j$ are the $x$ satisfying\n",
    "$$\n",
    "  (\\Sigma^{-1} (\\mu_{i} - \\mu_{j}))^{T} x =\n",
    "  \\half \\mu_{i}^{T} \\Sigma^{-1} \\mu_{i} - \\log P(E_{i}) -\n",
    "  \\half \\mu_{j}^{T} \\Sigma^{-1} \\mu_{j} + \\log P(E_{j}),\n",
    "$$\n",
    "which is a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training an LDA model\n",
    "\n",
    "There is no optimization involved in building an LDA model.  Instead, we use our sample (training data) to estimate the quantities involved.\n",
    "\n",
    "Let\n",
    "* $K$ be the number of classes with associated labels $1, 2, \\ldots, K$,\n",
    "* $N_{k}$ the number of instances of class $k$,\n",
    "* $N$ the total number of instances, and \n",
    "* $C_{k}$ be the set of instances $x$ with label $k$.\n",
    "\n",
    "Then\n",
    "\\begin{align*}\n",
    "  P(E_{i}) &= N_{i}/N \\\\\n",
    "  \\mu_{i} &= \\sum_{x \\in C_{i}} x/N_{i} \\\\\n",
    "  \\Sigma &= \\sum_{k=1}^{K} \\sum_{x \\in C_{i}} (x - \\mu_{i})(x - \\mu_{i})^{T}/(N-K).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic discriminant analysis\n",
    "\n",
    "In quadratic discriminant analysis (QDA) we allow different $\\Sigma_{k}$.  This leads to the decision functions\n",
    "$$\n",
    "  d_{i}(x) = - \\half \\log\\;\\abs{\\Sigma_{i}^{-1}}  - \\half (x - \\mu_{i}) \\Sigma^{-1} (x - \\mu_{i}) + \\log P(E_{i}).\n",
    "$$    \n",
    "The decision boundaries are now quadratic surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Logistic regression computes probabilities of class membership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary logistic regression\n",
    "\n",
    "Two classes: $C_{0}$ and $C_{1}$. \n",
    "\n",
    "Let $p(x)$ be the probability that $x$ belongs to class $C_{0}$.\n",
    "\n",
    "Since $p$ is a probability, we must have\n",
    "$$\n",
    "  0 \\leq p(x) \\leq 1.\n",
    "$$\n",
    "\n",
    "In order to model $p(x)$ we need a function that ranges between $0$ and $1$.\n",
    "\n",
    "One such function is the **logistic function** or **sigmoidal function**:\n",
    "$$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "which maps $\\R$ to $(0,1)$.\n",
    "\n",
    "In logistic regression we choose $w_{0}, w_{1}, \\ldots, w_{n}$ and model the probability as\n",
    "$$\n",
    "  p(x) = \\frac{1}{1 + e^{w_{0} + w_{1}x_{1} + \\cdots + w_{n}x_{n}}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation\n",
    "\n",
    "One approach to the training problem for logistic regression is **maximum likelihood estimation** (MLE).\n",
    "\n",
    "Suppose that when we drop a piece of buttered toast, we have\n",
    "\\begin{align*}\n",
    "  p(\\mbox{toast lands buttered side down}) &= \\theta, \\\\\n",
    "  p(\\mbox{toast lands buttered side up})   &= 1-\\theta.\n",
    "\\end{align*}\n",
    "\n",
    "This is a probability model that depends on the parameter $\\theta$.  Now suppose we drop 10 pieces of toast\n",
    "and observe the sequence\n",
    "<blockquote>\n",
    "  up, down, down, down, up, up, down, up, down, down.\n",
    "</blockquote>\n",
    "Since 6 pieces land buttered side down, and 4 pieces land buttered side up, we surmise\n",
    "$$\n",
    "  \\theta = \\frac{6}{6+4} = 0.6.\n",
    "$$\n",
    "\n",
    "If our experiment were described by $p(\\mbox{down}) = \\theta$, $p(\\mbox{up}) = 1-\\theta$, then the\n",
    "**likelihood** we would see that particular sequence of results is\n",
    "$$\n",
    "  L(\\theta) = (1-\\theta) \\theta \\theta \\theta (1-\\theta) (1-\\theta) \\theta (1-\\theta) \\theta \\theta\n",
    "  = \\theta^{6} (1-\\theta)^{4}.\n",
    "$$\n",
    "This is the probability of observing this particular sequence of outcomes provided that $\\theta$ was the probability that toast lands buttered side up.\n",
    "\n",
    "For which $\\theta$ is the outcome we saw most likely?  That is, what is the $\\theta$ that gives us the maximum likelihood?\n",
    "\n",
    "Since $\\theta$ is a probability, we know that $0 \\leq \\theta \\leq 1$, so we are asking about the solution of\n",
    "$$\n",
    "  \\begin{array}{ll}\n",
    "    \\mbox{maximize} & L(\\theta) \\\\\n",
    "    \\mbox{subject to} & 0 \\leq \\theta \\leq 1.\n",
    "  \\end{array}\n",
    "$$\n",
    "In this case we have\n",
    "$$\n",
    "  L'(\\theta) = 6 \\theta^{5} (1-\\theta)^{4} - 4 \\theta^{6} (1-\\theta)^{3}\n",
    "  = \\theta^{5} (1-\\theta)^{3} (6 (1-\\theta) - 4 \\theta).\n",
    "$$\n",
    "Thus, $L'(\\theta) = 0$ when $\\theta = 0, 1$, and\n",
    "$$\n",
    "  6 (1-\\theta) - 4 \\theta = 6 - 10 \\theta = 0,\n",
    "$$\n",
    "or $\\theta = 0.6$.\n",
    "\n",
    "Inspecting these candidates, we find that $\\theta = 0.6$ yields the maximum likelihood, which is consistent with\n",
    "our previous reasoning.\n",
    "\n",
    "The general idea in MLE is that we have observations that are presumed to be drawn from a probability distribution parameterized by unknown parameters $\\theta$.  We then choose the $\\theta$ so that the likelihood of our observations is maximized if they were drawn with the distribution with parameter values $\\theta$.\n",
    "\n",
    "The following function, which is defined in terms of the training data $x_{i}$ and the associated class labels, is called the **likelihood function** for this problem:\n",
    "$$\n",
    "  L(w_{0}, w_{1}, \\ldots, w_{n}) = \\prod_{x_{i} \\in C_{0}} p(x_{i}) \\prod_{x_{i} \\in C_{1}} (1-p(x_{i})).\n",
    "$$\n",
    "The goal in MLE is to choose model parameters to maximize this function.\n",
    "\n",
    "To avoid problems like numerical underflow to zero, it is customary to take the logarithm of the likelihood function (the so-called **log-likelihood**. For binary logistic regression the log-likelihood is\n",
    "$$\n",
    "  \\ell(w_{0}, w) = \\log L(w_{0}, w)\n",
    "  = \\sum_{x_{i} \\in C_{0}} \\ln p(x_{i}) + \\sum_{x_{i} \\in C_{1}} \\ln (1-p(x_{i})).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher's iris data set\n",
    "\n",
    "We will use Fisher's iris data set again to illustrate these linear classifiers.  This is only fitting as Fisher introduced the iris data set in the same paper where he presented linear discriminant analysis.\n",
    "\n",
    "The data set consists of 50 samples from each of three species of iris, \n",
    "* [*I. setosa*](https://en.wikipedia.org/wiki/Iris_setosa),\n",
    "* [*I. versicolor*](https://en.wikipedia.org/wiki/Iris_versicolor), and \n",
    "* [*I. virginica*](https://en.wikipedia.org/wiki/Iris_virginica).\n",
    "  \n",
    "for a total of 150 instances.\n",
    "\n",
    "Four **features** are measured for each sample: \n",
    "1. the length of the sepal,\n",
    "2. the width of the sepal,\n",
    "3. the length of the petal, and\n",
    "4. the width of the petal.\n",
    "\n",
    "The measurements are in centimeters.\n",
    "\n",
    "The **class labels** are\n",
    "1. Iris setosa,\n",
    "2. Iris versicolor,\n",
    "3. Iris virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"Class labels:\", np.unique(y))\n",
    "print(\"Class names:\", iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.feature_names)\n",
    "print(X[0:10,:])  # Just the first 10 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use only the petal length and width so we will be able to plot the decision regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test sets\n",
    "\n",
    "Split the data into 70% training and 30% test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y, test_size=0.3, random_state=0)  # Be sure to set the random seed so the results are reproducible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lda.fit(X_train, y_train)\n",
    "qda.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Set clf to be the classifier we wish to study.\n",
    "clf = qda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hacked up version of https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Parameters\n",
    "num_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "\n",
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[2:],\n",
    "    ylabel=iris.feature_names[2:],\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_train == i)\n",
    "    plt.scatter(\n",
    "        X_train[idx, 0],\n",
    "        X_train[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15,\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the tree showing the training data\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that QDA can produce complex nonlinear shapes in its decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_train)\n",
    "print(f\"Misclassified training samples: {(y_train != y_pred).sum()}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Misclassified test samples: {(y_test != y_pred).sum()}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# The confusion matrix for the training data.\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "class_names=(\"I. setosa\", \"I. versicolor\", \"I. virginica\")\n",
    "ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, display_labels=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[2:],\n",
    "    ylabel=iris.feature_names[2:],\n",
    ")\n",
    "\n",
    "# Plot the test points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_test == i)\n",
    "    plt.scatter(\n",
    "        X_test[idx, 0],\n",
    "        X_test[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15,\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the tree showing the test data\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
