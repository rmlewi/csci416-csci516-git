{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "    <h1>CSCI 416-01/516-01, Fundamentals of AI/ML, Fall 2025</h1>\n",
    "    <h1>Support vector classifiers</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents \n",
    "\n",
    "- [Linear classifiers](#Linear-classifiers)\n",
    "- [Characterization of separating hyperplanes](#Characterization-of-separating-hyperplanes)\n",
    "- [Modifying the separability condition](#Modifying-the-separability-condition)\n",
    "- [Which separator is right for you?](#Which-separator-is-right-for-you?)\n",
    "- [Finding the optimal separating hyperplane](#Finding-the-optimal-separating-hyperplane)\n",
    "- [Overlapping classes](#Overlapping-classes)\n",
    "    - [Relaxing the separation constraint](#Relaxing-the-separation-constraint)\n",
    "    - [The maximum margin SVC](#The-maximum-margin-SVC)\n",
    "- [Parameters vs hyperparameters](#Parameters-vs-hyperparameters)\n",
    "- [Building an SVC in Scikit-Learn](#Building-an-SVC-in-Scikit-Learn)\n",
    "- [Evaluating a classifier](#Evaluating-a-classifier)\n",
    "    - [Accuracy](#Accuracy)\n",
    "    - [True positive and false positive rates](#True-positive-and-false-positive-rates)\n",
    "    - [The confusion matrix](#The-confusion-matrix)\n",
    "    - [Precision and recall](#Precision-and-recall)\n",
    "    - [The $F$-score](#The-$F$-score)\n",
    "    - [Sensitivity and specificity](#Sensitivity-and-specificity)\n",
    "- [Evaluating the iris SVC](#Evaluating-the-iris-SVC)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Linear classifiers\n",
    "\n",
    "The goal of classification is to divide the input space into **decision regions** with a single class in each region.\n",
    "\n",
    "The boundaries of the decision regions are called the **decision boundaries** or **decision surfaces**.\n",
    "\n",
    "In **linear classification**, the decision surfaces are hyperplanes.  In $n$ dimensions, a hyperplane has the form\n",
    "$\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\Rn}{\\R^{n}}\n",
    "$\n",
    "$$\n",
    "    \\{x \\in \\Rn \\;|\\; w_{1}x_{1} + \\cdots + w_{n}x_{n} + b = w^{T} x + b = 0\\}\n",
    "$$\n",
    "for some $w \\in \\Rn$ and $b \\in \\R$.\n",
    "\n",
    "Thus, to build a linear separator for binary classification with classes $C_{-1}$ and $C_{+1}$, we seek a scalar $b$ and vector $w$ such that \n",
    "\\begin{align*}\n",
    "  w^{T}x^{(i)} + b &> 0\n",
    "\\end{align*}\n",
    "for all $x^{(i)} \\in C_{+1}$, and\n",
    "\\begin{align*}\n",
    "  w^{T}x^{(i)} + b &< 0\n",
    "\\end{align*}\n",
    "for all $x^{(i)} \\in C_{-1}$.\n",
    "\n",
    "For binary classification, applying the classifier is, in principle, a matter of computing $w^{T}x + b$ and looking at the sign of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterization of separating hyperplanes\n",
    "\n",
    "**Lemma.**\n",
    "Given two finite sets $C_{-1}, C_{+1} \\subset \\Rn$, there exist $w \\in \\Rn$ and $b \\in \\R$ such that\n",
    "\\begin{align*}\n",
    "  w^{T}x^{(i)} + b &> 0 \\quad \\mbox{if $x^{(i)} \\in C_{+1}$}, \\\\\n",
    "  w^{T}x^{(i)} + b &< 0 \\quad \\mbox{if $x^{(i)} \\in C_{-1}$},\n",
    "\\end{align*}\n",
    "if and only if there exist $v \\in \\Rn$ and $c \\in \\R$ such that\n",
    "\\begin{align*}\n",
    "  v^{T}x^{(i)} + c &\\geq +1 \\quad \\mbox{if $x^{(i)} \\in C_{+1}$}, \\\\\n",
    "  v^{T}x^{(i)} + c &\\leq -1 \\quad \\mbox{if $x^{(i)} \\in C_{-1}$},\n",
    "\\end{align*}\n",
    "with $|\\;v^{T}x^{(i)} + c\\;| = 1$ for at least one $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the separability condition \n",
    "\n",
    "In light of the lemma, we can replace the original separability condition\n",
    "\\begin{align*}\n",
    "  w^{T}x^{(i)} + b &> 0 \\quad \\mbox{if $x^{(i)} \\in C_{+1}$}, \\\\\n",
    "  w^{T}x^{(i)} + b &< 0 \\quad \\mbox{if $x^{(i)} \\in C_{-1}$},\n",
    "\\end{align*}\n",
    "with the condition\n",
    "\\begin{align*}\n",
    "  w^{T}x^{(i)} + b &\\geq +1 \\quad \\mbox{if $x^{(i)} \\in C_{+1}$}, \\\\\n",
    "  w^{T}x^{(i)} + b &\\leq -1 \\quad \\mbox{if $x^{(i)} \\in C_{-1}$}.\n",
    "\\end{align*}\n",
    "\n",
    "In general, if we have two classes $C_{+1}$ and $C_{-1}$, let\n",
    "$$\n",
    "  y_{i} = \\left\\{\n",
    "    \\begin{array}{cl}\n",
    "      +1 & \\mbox{if $x^{(i)} \\in C_{+1}$,} \\\\\n",
    "      -1 & \\mbox{if $x^{(i)} \\in C_{-1}$.} \n",
    "    \\end{array}\n",
    "  \\right.\n",
    "$$\n",
    "The property \n",
    "\\begin{align*}\n",
    "  w^{T}x^{(i)} + b &\\geq +1 \\quad \\mbox{if $x^{(i)} \\in C_{+1}$}, \\\\\n",
    "  w^{T}x^{(i)} + b &\\leq -1 \\quad \\mbox{if $x^{(i)} \\in C_{-1}$},\n",
    "\\end{align*}\n",
    "is then equivalent to\n",
    "$$\n",
    "  y_{i} (w^{T}x^{(i)} + b) \\geq +1 \\quad \\mbox{for all $i$}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which separator is right for you?\n",
    "\n",
    "Let's grab Fisher's iris data once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "all_features = {'sepal_length':0, 'sepal_width':1, 'petal_length':2, 'petal_width':3}\n",
    "\n",
    "excluded = 'versicolor'  # Any species to exclude.\n",
    "features = ['sepal_width', 'petal_length']  # Features to be used.\n",
    "\n",
    "columns = [all_features[f] for f in features]\n",
    "print('Columns:', columns)\n",
    "    \n",
    "X = iris.data[:, columns]\n",
    "y = iris.target\n",
    "\n",
    "classes = {'setosa':0, 'versicolor':1, 'virginica':2, 'none':0}\n",
    "\n",
    "# Grab only rows for the non-excluded species.\n",
    "X = X[np.where(y != classes[excluded])]\n",
    "y = y[np.where(y != classes[excluded])]    \n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y, test_size=0.3, random_state=0)  # Be sure to set the random seed so the results are reproducible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "num_classes = 3\n",
    "plot_colors = \"yrb\"\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_train == i)\n",
    "    plt.scatter(\n",
    "        X_train[idx, 0],\n",
    "        X_train[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15)\n",
    "\n",
    "\n",
    "plt.plot([1, 5], [0, 5], color='c', linestyle='-', linewidth=2)\n",
    "plt.plot([1, 5], [2, 2], color='m', linestyle='-', linewidth=2)\n",
    "plt.plot([1, 5], [2.25, 4.25],  color='g', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we want a separating hyperplane that is as most \"in the middle\" as possible to avoid misclassification.\n",
    "\n",
    "Given any hyperplane, one of the $x^{(k)}$ is closest to the hyperplane.\n",
    "\n",
    "Let's choose the separating hyperplane that maximizes this minimum distance:\n",
    "$$\n",
    "\\max_{H \\in \\mbox{\\scriptsize hyperplanes}} \\min_{k}\\ \\{ \\mbox{distance from $x^{(k)}$ to H} \\}.\n",
    "$$\n",
    "\n",
    "We will measure distance in the Euclidean norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Finding the optimal separating hyperplane\n",
    "\n",
    "The Euclidean distance from a point $x^{(i)}$ to the hyperplane\n",
    "$$\n",
    "  \\{z \\in \\Rn \\;|\\; w^{T}z + b = 0\\}\n",
    "$$\n",
    "is\n",
    "$$\n",
    "  \\newcommand{\\abs}[1]{|\\; #1 \\;|}\n",
    "  \\newcommand{\\norm}[1]{\\|\\; #1 \\;\\|}\n",
    "  \\newcommand{\\twonorm}[1]{\\norm{#1}_{2}}\n",
    "  \\frac{\\abs{w^{T}x^{(i)} + b}}{\\twonorm{w}} = \\frac{y_{i}(w^{T}x^{(i)} + b)}{\\norm{w}}.\n",
    "$$\n",
    "\n",
    "The minimum distance from the $x^{(i)}$ to the hyperplane is thus\n",
    "$$\n",
    "  \\min_{i} \\frac{y_{i}(w^{T}x^{(i)} + b)}{\\norm{w}}.\n",
    "$$\n",
    "\n",
    "So, the problem we want to solve is\n",
    "$\n",
    "  \\DeclareMathOperator*{\\minimize}{\\mbox{minimize}}\n",
    "  \\DeclareMathOperator*{\\maximize}{\\mbox{maximize}}\n",
    "  \\DeclareMathOperator*{\\subjectto}{\\mbox{subject to}}\n",
    "$\n",
    "\\begin{align*}\n",
    "  \\maximize_{w,b} &\\quad \\min_{i} \\left(y_{i} \\frac{w^{T}x^{(i)} + b}{\\norm{w}}\\right) \\\\\n",
    "  \\subjectto &\\quad y_{i} (w^{T}x^{(i)} + b) \\geq 1 \\quad \\mbox{for all $i$}.\n",
    "\\end{align*}\n",
    "\n",
    "Moreover, from our lemma we can arrange that $y_{i} (w^{T}x^{(i)} + b) = 1$ for at least one $i$, so\n",
    "$$\n",
    "  \\min_{i} \\left(y_{i} \\frac{w^{T}x^{(i)} + b}{\\norm{w}}\\right) = \\frac{1}{\\norm{w}}.\n",
    "$$\n",
    "\n",
    "Thus we arrive at\n",
    "$$ \n",
    "  \\begin{array}{ll}\n",
    "    \\maximize_{w,b} & \\displaystyle \\frac{1}{\\norm{w}} \\\\\n",
    "    \\subjectto & y_{i} (w^{T}x^{(i)} + b) \\geq 1 \\quad \\mbox{for all $i$},\n",
    "  \\end{array}\n",
    "$$\n",
    "or, equivalently,\n",
    "$$\n",
    "  \\newcommand{\\half}{\\frac{1}{2}}\n",
    "  \\newcommand{\\normsq}[1]{\\norm{#1}^{2}}\n",
    "  \\begin{array}{ll}\n",
    "    \\minimize_{w,b} & \\half \\normsq{w} \\\\\n",
    "    \\subjectto & y_{i} (w^{T}x^{(i)} + b) \\geq 1 \\quad \\mbox{for all $i$}.\n",
    "  \\end{array}\n",
    "$$\n",
    "\n",
    "This is a quadratic program (QP) &ndash; minimization of a quadratic function subject to linear inequality constraints.\n",
    "\n",
    "The optimal separating hyperplane is given by the solution of this QP.\n",
    "\n",
    "Suppose we have solved\n",
    "$$\n",
    "  \\begin{array}{ll}\n",
    "    \\minimize_{w,b} & \\half \\twonorm{w}^{2} \\\\\n",
    "    \\subjectto & y_{i} (w^{T}x^{(i)} + b) \\geq 1 \\quad \\mbox{for all $i$}.\n",
    "  \\end{array}\n",
    "$$\n",
    "\n",
    "A **support vector** is any point in either class for which \n",
    "$$\n",
    "  y_{i} (w^{T}x^{(i)} + b) = 1.\n",
    "$$\n",
    "\n",
    "Support vectors are the points in the training set closest to the separating hyperplane.\n",
    "\n",
    "The solution of the QP defines a **support vector classifier** (we will generalize this shortly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlapping classes\n",
    "\n",
    "If the classes overlap, then it is impossible to satisfy all of the constraints.\n",
    "\n",
    "A constrained optimization problem that has constraints that cannot be satisfied is called **infeasible**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = 'setosa'  # Any species to exclude.\n",
    "    \n",
    "X = iris.data[:, columns]\n",
    "y = iris.target\n",
    "\n",
    "# Grab only rows for the non-excluded species.\n",
    "X = X[np.where(y != classes[excluded])]\n",
    "y = y[np.where(y != classes[excluded])]    \n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y, test_size=0.3, random_state=0)  # Be sure to set the random seed so the results are reproducible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_train == i)\n",
    "    plt.scatter(\n",
    "        X_train[idx, 0],\n",
    "        X_train[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15)\n",
    "\n",
    "\n",
    "# plt.plot([1, 5], [5, 5], color='c', linestyle='-', linewidth=2)\n",
    "# plt.plot([1, 5], [4, 6], color='m', linestyle='-', linewidth=2)\n",
    "# plt.plot([1, 5], [2.25, 4.25],  color='g', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relaxing the separation constraint\n",
    "One approach to sets that are not cleanly linearly separable is to allow some misclassification, but to penalize for it.\n",
    "\n",
    "Introduce new variables $\\xi_{i} \\geq 0$ to the problem, and relax the separability constraints from\n",
    "$$\n",
    "  \\begin{array}{ll}\n",
    "    \\subjectto & y_{i} (w^{T}x^{(i)} + b) \\geq 1 \\quad \\mbox{for all $i$}.\n",
    "  \\end{array}\n",
    "$$\n",
    "to \n",
    "$$\n",
    "  \\begin{array}{ll}\n",
    "    \\subjectto & y_{i} (w^{T}x^{(i)} + b) \\geq 1 - \\xi_{i} \\quad \\mbox{for all $i$}.\n",
    "  \\end{array}\n",
    "$$\n",
    "We can ensure that there exist $w$ and $b$ satisfying the latter constraints if the values of  the $\\xi_{i}$ are sufficiently large.\n",
    "\n",
    "The $\\xi_{i}$ are a version of **slack variables**---they allow us to relax a condition by allowing slack in its satisfaction.\n",
    "\n",
    "When we have solved the QP, there are three possibilities for each training case $i$:\n",
    "1. if $\\xi_{i} = 0$ the case is properly classified and safely away from the separating hyperplane;\n",
    "2. if $0 < \\xi_{i} \\leq 1$ the case is properly classified, but is within distance\n",
    "   $$\n",
    "      \\frac{1 - \\xi_{i}}{\\norm{w}}\n",
    "   $$\n",
    "   of the separating hyperplane;\n",
    "3. if $\\xi_{i} > 1$ the case is incorrectly classified, since it lies on the wrong side of the separating hyperplane.\n",
    "\n",
    "A **margin error** is a point whose corresponding $\\xi_{i}$ is greater than $0$.  These points lie on the wrong side of the margin around the separating hyperplane and which are potentially (but not necessarily) misclassified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The maximum margin SVC\n",
    "\n",
    "However, we don't want the $\\xi_{i}$ to be too large, since that allows a greater proportion of misclassfication, so we'll penalize for large values of $\\xi_{i}$.\n",
    "\n",
    "This leads to a relaxed version of the problem called the **maximum margin problem**.\n",
    "  \n",
    "Choose a weight $C > 0$, and solve\n",
    "$$\n",
    "  \\DeclareMathOperator*{\\minimize}{\\mbox{minimize}}\n",
    "  \\DeclareMathOperator*{\\maximize}{\\mbox{maximize}}\n",
    "  \\DeclareMathOperator*{\\subjectto}{\\mbox{subject to}}\n",
    "  \\begin{array}{ll}\n",
    "    \\minimize_{w,b,\\xi} & \\half \\twonorm{w}^{2} + C \\sum_{i} \\xi_{i} \\\\\n",
    "    \\subjectto & y_{i} (w^{T}x^{(i)} + b) \\geq 1 - \\xi_{i} \\quad \\mbox{for all $i$} \\\\\n",
    "               & \\xi_{i} \\geq 0 \\quad \\mbox{for all $i$}.\n",
    "  \\end{array}\n",
    "$$\n",
    "\n",
    "Suppose we have\n",
    "* $n$ feature variables in $x$, and\n",
    "* $N$ feature vectors in our training set.\n",
    "\n",
    "Then the maximum margin QP has\n",
    "* $n+N+1$ optimization variables $w, \\mathbf{\\xi}, b$, and\n",
    "* $2N$ constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters vs hyperparameters\n",
    "\n",
    "To construct the SVC, we must the choose a weight $C > 0$:\n",
    "$$\n",
    "  \\begin{array}{ll}\n",
    "    \\minimize_{w,b,\\xi} & \\half \\twonorm{w}^{2} + C \\sum_{i} \\xi_{i} \\\\\n",
    "    \\subjectto & y_{i} (w^{T}x^{(i)} + b) \\geq 1 - \\xi_{i} \\quad \\mbox{for all $i$} \\\\\n",
    "               & \\xi_{i} \\geq 0 \\quad \\mbox{for all $i$}.\n",
    "  \\end{array}\n",
    "$$\n",
    "In this context, the $w, b$ are **parameters** in the SVC, while $C$ is a **hyperparameter** used to\n",
    "construct the SVC.\n",
    "* The choice of hyperparameters affects the choice of parameters for our ML method.\n",
    "* In turn, the choice of parameters affects the performance of our ML method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an SVC in Scikit-Learn\n",
    "\n",
    "We will use the scikit-learn SVC (support vector classifier) module.  SVC is based on [libSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm), which is also used in the R package [e1071](https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf).\n",
    "\n",
    "The documentation is [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "We will use all three iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, columns]\n",
    "y = iris.target\n",
    "\n",
    "# Grab only rows for the non-excluded species.\n",
    "# X = X[np.where(y != classes[excluded])]\n",
    "# y = y[np.where(y != classes[excluded])] \n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y, test_size=0.3, random_state=0)  # Be sure to set the random seed so the results are reproducible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# The choice kernel='linear' is important.\n",
    "svc = SVC(kernel='linear', C=0.1, random_state=0)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "N, n  = X_train.shape\n",
    "print(\"Size of QP:\")\n",
    "print(f\"  Number of variables:   {n + N + 1}\")\n",
    "print(f\"  Number of constraints: {2*N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    svc,\n",
    "    X,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[1:3],\n",
    "    ylabel=iris.feature_names[1:3]\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = np.where(y_train == i)\n",
    "    plt.scatter(\n",
    "        X_train[idx, 0],\n",
    "        X_train[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the SVC showing the training data\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the support vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries.\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    svc,\n",
    "    X,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[1:3],\n",
    "    ylabel=iris.feature_names[1:3]\n",
    ")\n",
    "\n",
    "# Plot the support vectors.\n",
    "for i, color in zip(range(num_classes), plot_colors):\n",
    "    idx = list(np.where(y_train == i))\n",
    "    idx = list(set(list(idx[0])).intersection(svc.support_))\n",
    "    plt.scatter(\n",
    "        X_train[idx, 0],\n",
    "        X_train[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Decision boundaries of the SVC showing the support vectors\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a classifier\n",
    "\n",
    "How should we evaluate classifiers?  How should we compare the effectiveness of classifiers?\n",
    "\n",
    "There is a bewildering variety of performance metrics for binary classifiers.\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "One obvious metric is **accuracy**:\n",
    "$$\n",
    "  \\mbox{accuracy} = \\frac{\\mbox{number of correctly classified test cases}}{\\mbox{number of test cases}}.\n",
    "$$\n",
    "The misclassification rate (error rate) is 1 - accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = svc.predict(X_test)\n",
    "print(f\"Misclassified test samples: {(y_test != y_pred).sum()}\")\n",
    "print(f\"Accuracy on test set: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_train)\n",
    "print(f\"Misclassified training samples: {(y_train != y_pred).sum()}\")\n",
    "print(f\"Accuracy on training set: {accuracy_score(y_train, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, high accuracy might not be what we want:\n",
    "* is it better for a spam filter to let some spam through than to block important messages?\n",
    "* is it better for a cancer test err on the side of caution and tell you you have cancer when you don't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True positive and false positive rates\n",
    "\n",
    "Consider a two-class problem with classes **positive** and **negative**, denoted by $\\oplus$ and $\\ominus$, respectively.\n",
    "\n",
    "Correctly classified positives and negatives are called **true positives** and **true negatives**.\n",
    "\n",
    "Incorrectly classified positives are called **false negatives** (false alarms), while incorrectly classified negatives are called **false positives**.\n",
    "\n",
    "In true/false positive/negative, \n",
    "* positive/negative refers to the classifier's prediction, while \n",
    "* true/false refers to whether this prediction is correct.\n",
    "\n",
    "The **true positive rate** (TPR) and **false positive rate** (FPR) are defined as follows.  Let\n",
    "\\begin{align*}\n",
    "  P  &= \\mbox{true number of $\\oplus$'s}, \\\\\n",
    "  N  &= \\mbox{true number of $\\ominus$'s}, \\\\\n",
    "  TP &= \\mbox{number of true positives}, \\\\\n",
    "  TN &= \\mbox{number of true negatives}, \\\\\n",
    "  FP &= \\mbox{number of false positives}, \\\\\n",
    "  FN &= \\mbox{number of false negatives}.\n",
    "\\end{align*}\n",
    "Then\n",
    "\\begin{align*}\n",
    "  TPR &= \\frac{TP}{T} = \\frac{TP}{FN + TP}, \\\\\n",
    "  FPR &= \\frac{FP}{N} = \\frac{FP}{FP + TN}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The confusion matrix\n",
    "\n",
    "A **two-class contingency table** is the confusion matrix in the case of binary classification:\n",
    "<table>\n",
    "<tr><th></th><th>classified as $\\oplus$</th> <th>classified as $\\ominus$</th>\n",
    "<tr><td>actually $\\oplus$</td><td style=\"color:blue;\"><b>60</b></td> <td style=\"color:red;\"><b>15</b></td>\n",
    "<tr><td>actually $\\ominus$</td><td style=\"color:red;\"><b>10</b></td><td style=\"color:blue;\"><b>15</b></td>\n",
    "</table>\n",
    "The NW-SE diagonal shows the correctly classified cases, while the SW-NE diagonal shows the incorrectly classified cases.  The numbers on the borders are the row and column sums.\n",
    "\n",
    "More generally, the confusion matrix will give some details of who is being misclassified as what.  This allows us to look for systematic misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and recall\n",
    "\n",
    "Again consider a two-class problem with classes **positive** and **negative**, denoted by\n",
    "$\\oplus$ and $\\ominus$, respectively.\n",
    "\n",
    "**Precision**: of all the $\\oplus$'s you found, how fraction of them were really $\\oplus$'s? \n",
    "\n",
    "**Recall**: of all the $\\oplus$'s that are really there, what fraction of them did you find? \n",
    "\n",
    "In terms of our earlier notation, the precision $P$ and recall $R$ are given by\n",
    "$$\n",
    "  P = \\frac{TP}{TP + FP}, \\quad R = \\frac{TP}{P} = \\frac{TP}{FN + TP}.\n",
    "$$\n",
    "\n",
    "$P$ and $R$ lie in the interval $[0,1]$.  The closer the values are to 1, the better the classifier.\n",
    "\n",
    "Note that recall is the same as the true positive rate.\n",
    "\n",
    "For the positives the confusion matrix\n",
    "<table>\n",
    "<tr><th></th><th>predicted $\\oplus$</th> <th>predicted $\\ominus$</th>\n",
    "<tr><td>actually $\\oplus$</td><td style=\"color:blue;\"><b>60</b></td> <td style=\"color:red;\"><b>15</b></td>\n",
    "<tr><td>actually $\\ominus$</td><td style=\"color:red;\"><b>10</b></td><td style=\"color:blue;\"><b>15</b></td>\n",
    "</table>\n",
    "are\n",
    "\\begin{align*}\n",
    "  TP &= 60 \\\\\n",
    "  FP &= 10 \\\\\n",
    "  FN &= 15 \\\\\n",
    "  P &= TP/(TP + FP) = 60/70 = 0.857 \\\\\n",
    "  R &= TP/(FN + TP) = 60/75 = 0.8.\n",
    "\\end{align*}\n",
    "\n",
    "For the positives, this classifier has both good precision and good recall.\n",
    "\n",
    "### Alternative definition\n",
    "\n",
    "The use of the terms *positive* and *negative* can sometimes be confusing.\n",
    "\n",
    "More generally, suppose $C$ is one of our classes.  Let\n",
    "\\begin{align*}\n",
    "  F &= \\mbox{number of cases classified as $C$'s}, \\\\\n",
    "  T &= \\mbox{true number of $C$'s in our test set}, \\\\\n",
    "  B &= \\mbox{number of cases that are both in $C$ and classified as such}.\n",
    "\\end{align*}\n",
    "\n",
    "Then the precision $P$ and recall $R$ for class $C$ are given by\n",
    "$$\n",
    "  P = B/F, \\quad R = B/T.\n",
    "$$\n",
    "\n",
    "For the negatives in the confusion matrix\n",
    "<table>\n",
    "<tr><th></th><th>classified as $\\oplus$</th> <th>classified as $\\ominus$</th>\n",
    "<tr><td>actually $\\oplus$</td><td style=\"color:blue;\"><b>60</b></td> <td style=\"color:red;\"><b>15</b></td>\n",
    "<tr><td>actually $\\ominus$</td><td style=\"color:red;\"><b>10</b></td><td style=\"color:blue;\"><b>15</b></td>\n",
    "</table>\n",
    "we have\n",
    "\\begin{align*}\n",
    "  F &= 30 \\\\\n",
    "  T &= 25 \\\\\n",
    "  B &= 15 \\\\\n",
    "  P &= B/F = 15/30 = 0.5 \\\\\n",
    "  R &= B/T = 15/25 = 0.6.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $F$-score\n",
    "\n",
    "The $F$-score combines precision and recall via harmonic averaging.\n",
    "\n",
    "The **harmonic average** $h(a,b)$ of $a$ and $b$ is the reciprocal of the average of their reciprocals:\n",
    "$$\n",
    "  \\newcommand{\\half}{\\frac{1}{2}}\n",
    "  \\frac{1}{\\half\\left(\\frac{1}{a} + \\frac{1}{b}\\right)},\n",
    "$$\n",
    "with the convention $1/0 = \\infty$.\n",
    "\n",
    "The harmonic mean has the following properties:\n",
    "1. if $a \\leq b$, then $a \\leq h(a,b) \\leq b$;\n",
    "2. $h(a,0) = h(0,b) = 0$;\n",
    "3. $h(a,a) = a$ (even if $a = 0$).\n",
    "\n",
    "Property 2 suggests that the harmonic mean weighs terms close to $0$ more heavily.\n",
    "\n",
    "One way to combine precision and recall into a single number is to take their harmonic mean:\n",
    "$$\n",
    "  F = \\frac{1}{\\half\\left(\\frac{1}{P} + \\frac{1}{R}\\right)}.\n",
    "$$\n",
    "This is called the **F-score** or **F1-score**.\n",
    "\n",
    "The harmonic mean favors a balance of roughly equal precision and recall.\n",
    "\n",
    "If $P = R$, then $F = P = R$.\n",
    "\n",
    "If $F = P = 1$, then $F = 1$.\n",
    "\n",
    "On the other hand, if $P = 0.05$ and $R = 0.9$, then $F = 0.0947$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# Make data.\n",
    "P = np.linspace(1.0e-16, 1)\n",
    "R = np.linspace(1.0e-16, 1)\n",
    "P, R = np.meshgrid(P, R)\n",
    "F = 1/(0.5*(1/P + 1/R))\n",
    "\n",
    "# Plot the surface.\n",
    "mesh = plt.pcolormesh(P, R, F, cmap=plt.cm.OrRd)\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(mesh)\n",
    "\n",
    "plt.xlabel('P')\n",
    "plt.ylabel('R')\n",
    "plt.title('Heatmap of F-score(P,R)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity and specificity\n",
    "\n",
    "Another evaluation metric is sensitivity/specificity.\n",
    "\n",
    "**Sensitivity** is the same thing as recall: do I almost always find everything I am looking for?  It is the **true positive rate**, the proportion of positive cases that are correctly classified.  This is identical to **recall**.\n",
    "\n",
    "**Specificity** refers to whether we do a good job not finding things we are not looking for.  It is the **true negative rate**, the proportion of negative cases that are correctly classified.\n",
    "\n",
    "For example, a good medical test is sensitive and specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the iris SVC\n",
    "\n",
    "Here we will use the confusion matrix, precision, recall, and $F$-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Results for the test set.\")\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Accuracy, precision, recall, and f-score.\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# The confusion matrix for the training data.\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "class_names=(\"I. setosa\", \"I. versicolor\", \"I. virginica\")\n",
    "ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test, display_labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The preceding shows the misclassification of the test data.  But how did the SVC do on the training data?\n",
    "\n",
    "Let's find out&hellip;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Results for the test set.\")\n",
    "y_pred = svc.predict(X_train)\n",
    "\n",
    "# Accuracy, precision, recall, and f-score.\n",
    "print(metrics.classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# The confusion matrix for the training data.\n",
    "y_pred = svc.predict(X_train)\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "class_names=(\"I. setosa\", \"I. versicolor\", \"I. virginica\")\n",
    "ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test, display_labels=class_names)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
