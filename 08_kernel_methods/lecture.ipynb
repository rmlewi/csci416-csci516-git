{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "    <h1>CSCI 416-01/516-01: Fundamentals of AI/ML, Fall 2025</h1>\n",
    "    <h1>Kernel methods</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Contents \n",
    "\n",
    "- [Support vector classifiers](#Support-vector-classifiers)\n",
    "- [The dual QP](#The-dual-QP)\n",
    "    - [The dual QP for overlapping classes](#The-dual-QP-for-overlapping-classes)\n",
    "- [Emedding in a higher dimension](#Emedding-in-a-higher-dimension)    \n",
    "- [The kernel trick](#The-kernel-trick)\n",
    "    - [The kernel for XOR](#The-kernel-for-xor)\n",
    "    - [¿ $\\phi(x)$ or $K(x,y)$ ?](#¿-$\\phi(x)$-or-$K(x,y)$-?)\n",
    "    - [Properties of kernels](#Properties-of-kernels)\n",
    "- [A necessary and sufficient condition for kernels](#A-necessary-and-sufficient-condition-for-kernels)\n",
    "- [The $\\nu$-SVC](#The-$\\nu$-SVC)\n",
    "- [Examples of kernels](#Examples-of-kernels)\n",
    "- [Constructing a kernel SVC in Scikit-Learn](#Constructing-a-kernel-SVC-in-Scikit-Learn)\n",
    "- [Kernels for symbolic inputs](#Kernels-for-symbolic-inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emedding in a higher dimension\n",
    "\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "\n",
    "**Kernel methods** provide a way of handling sets that are not linearly separable.\n",
    "\n",
    "If we can't linearly separate the original inputs $x^{(1)}, \\ldots, x^{(N)}$, perhaps we can linearly separate them after applying a nonlinear transformation to embed them in a higher-dimensional space.\n",
    "\n",
    "Let $\\phi(x)$ be the transformation, or **feature map**, that we apply to our inputs.  The function $\\phi$ may be nonlinear.\n",
    "\n",
    "The range of $\\phi$ is the **feature space**, while the space we map from is called **input space** in this context.\n",
    "\n",
    "XOR is an example of a set of points that are not linearly separable.  Let's map the inputs from $\\R^{2}$ to $\\R^{3}$ using the transformation\n",
    "$$\n",
    "  \\phi(x_{1}, x_{2}) = (x_{1}^{2}, x_{2}^{2}, \\sqrt{2}x_{1}x_{2}).\n",
    "$$\n",
    "Then\n",
    "\\begin{align*}\n",
    "  F = (+1, +1) &\\mapsto (+1, +1, +\\sqrt{2}) \\\\\n",
    "  T = (+1, -1) &\\mapsto (+1, +1, -\\sqrt{2}) \\\\\n",
    "  T = (-1, +1) &\\mapsto (+1, +1, -\\sqrt{2}) \\\\\n",
    "  F = (-1, -1) &\\mapsto (+1, +1, +\\sqrt{2}).\n",
    "\\end{align*}\n",
    "Note that our transformation is **not** one-to-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# mpl.rcParams['legend.fontsize'] = 12\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "x = [1, 1]\n",
    "y = [1, 1]\n",
    "z = [1.414, 1.414]\n",
    "ax.scatter(x, y, z, c=['r'])\n",
    "\n",
    "x = [1, 1]\n",
    "y = [1, 1]\n",
    "z = [-1.414, -1.414]\n",
    "ax.scatter(x, y, z, c=['b'])\n",
    "\n",
    "ax.legend(['F', 'T'])\n",
    "ax.set_xlabel('$x_{1}$')\n",
    "ax.set_ylabel('$x_{2}$')\n",
    "ax.set_zlabel('$x_{3}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah ha!  Now our two classes are on opposite sides of the $x_{1}-x_{2}$ plane!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector classifiers\n",
    "\n",
    "In order to derive kernel classifiers we must make a detour to the wonderful world of convex duality!\n",
    "\n",
    "Consider building an SVC with training cases $x^{(1)}, \\ldots, x^{(N)}$.\n",
    "\n",
    "Recall the QP for the optimal SVC hyperplane:\n",
    "$\\DeclareMathOperator*{\\minimize}{minimize}$\n",
    "$\\DeclareMathOperator*{\\subjectto}{subject to}$\n",
    "$\\newcommand{\\half}{\\frac{1}{2}}$\n",
    "$\\newcommand{\\norm}[1]{\\|\\; #1 \\;\\|}$\n",
    "$\\newcommand{\\twonormsq}[1]{\\norm{#1}_{2}^{2}}$\n",
    "\\begin{align*}\n",
    "    \\minimize_{w,b} &\\quad \\half \\twonormsq{w} \\\\\n",
    "    \\subjectto &\\quad y_{i} (w^{T}x^{(i)} + b) \\geq 1 \\quad \\mbox{for all $1 \\leq i \\leq t$}.\n",
    "\\end{align*}\n",
    "We will call this QP the **primal** problem.\n",
    "\n",
    "The **dual** QP is\n",
    "$\\DeclareMathOperator*{\\maximize}{maximize}$\n",
    "\\begin{align*}\n",
    "  \\maximize_{\\mu = (\\mu_{1}, \\ldots, \\mu_{N})} &\\quad \\sum_{i=1}^{N} \\mu_{i} \n",
    "       - \\half \\sum_{i=1}^{N}\\sum_{i=1}^{N} \\mu_{i}\\mu_{j}y_{i}y_{j}(x^{(i)})^{T}x^{(j)} \\\\\n",
    "  \\subjectto &\\quad \\mu_{i} \\geq 0,\\ i = 1, \\ldots, N \\\\\n",
    "             &\\quad \\sum_{i=1}^{N} \\mu_{i}y_{i} = 0.\n",
    "\\end{align*}\n",
    "\n",
    "The optimal value of the primal is the same as the duel.  Moreover, knowing the solution to one of the two enables to build a solution to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dual QP for overlapping classes\n",
    "\n",
    "Recall the relaxed maximum margin problem for overlapping classes (i.e., classes that are not linearly separable):\n",
    "\\begin{align*}\n",
    "  \\minimize_{w,b,\\xi} &\\quad \\half \\twonormsq{w} + C \\sum_{i=1}^{N} \\xi_{i} \\\\\n",
    "  \\subjectto &\\quad y_{i} (w^{T}x^{(i)} + b) \\geq 1 - \\xi_{i}, \\quad i = 1, \\ldots N \\\\\n",
    "             &\\quad \\xi_{i} \\geq 0, \\quad i = 1, \\ldots, N.\n",
    "\\end{align*}\n",
    "\n",
    "This QP has the dual\n",
    "\\begin{align*}\n",
    "  \\maximize_{\\mu = (\\mu_{1}, \\ldots, \\mu_{N})} &\\quad \\sum_{i=1}^{N} \\mu_{i} \n",
    "      - \\half \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\mu_{i}\\mu_{j}y_{i}y_{j}(x^{(i)})^{T} x^{(j)} \\\\\n",
    "  \\subjectto &\\quad 0 \\leq \\mu_{j} \\leq C,\\ i = 1, \\ldots, N \\\\\n",
    "            &\\quad \\sum_{i=1}^{N} \\mu_{i} y_{i} = 0.\n",
    "\\end{align*}\n",
    "\n",
    "The difference between this dual and the previous dual for linearly separable classes is the upper bound on the $\\mu_{i}$.\n",
    "\n",
    "The solution of this problem is used to build an SVC just as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The kernel trick\n",
    "\n",
    "The optimal separating hyperplane in the feature space is found by solving\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\minimize_{w,b,\\xi} &\\qquad \\half \\twonormsq{w} + C \\sum_{i} \\xi_{i} \\\\\n",
    "    \\subjectto &\\qquad y_{i} (w^{T}\\mathbf{\\phi}(x^{(i)}) + b) \\geq 1 - \\xi_{i} \\quad \\mbox{for all $i$} \\\\\n",
    "               &\\qquad \\xi_{i} \\geq 0 \\quad \\mbox{for all $i$}.\n",
    "\\end{align*}\n",
    "$$\n",
    "or the dual,\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\maximize_{\\mu_{1}, \\ldots, \\mu_{N}} &\\qquad \\sum_{i=1}^{N} \\mu_{i} \n",
    "       - \\half \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\mu_{j} \\mu_{k} y_{j} y_{k} \\phi(x^{(i)})^{T} \\phi(x^{(j)}) \\\\\n",
    "    \\subjectto &\\qquad 0 \\leq \\mu_{i} \\leq C,\\ i = 1, \\ldots, N \\\\\n",
    "               &\\qquad \\sum_{i=1}^{N} \\mu_{j}y_{j} = 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "The dual turns out to be more interesting.\n",
    "\n",
    "The training data enter the dual through the inner product of the $\\phi(x^{(i)})$.   Let\n",
    "$$\n",
    "  K(x,y) = \\phi(x)^{T} \\phi(y).\n",
    "$$\n",
    "$K$ is called a **kernel**.\n",
    "\n",
    "Then the dual can be written as\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\maximize_{\\mu_{1}, \\ldots, \\mu_{N}} &\\qquad \\sum_{i=1}^{N} \\mu_{i} \n",
    "      - \\half \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\mu_{j} \\mu_{k} y_{j} y_{k} K(x^{(i)}, x^{(j)}) \\\\\n",
    "    \\subjectto &\\qquad \\mu_{i} \\geq 0,\\ i = 1, \\ldots, N \\\\\n",
    "               &\\qquad \\sum_{i=1}^{N} \\mu_{i}y_{i} = 0.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The kernel for XOR\n",
    "\n",
    "Recall the transformation we used for XOR:\n",
    "$$\n",
    "  \\mathbf{\\phi}(x_{1}, x_{2}) = (x_{1}^{2}, x_{2}^{2}, \\sqrt{2}x_{1}x_{2}).\n",
    "$$\n",
    "Observe that \n",
    "\\begin{align*}\n",
    "  \\mathbf{\\phi}(x_{1}, x_{2})^{T}\\mathbf{\\phi}(y_{1}, y_{2}) \n",
    "  &= \\begin{pmatrix} x_{1}^{2} & x_{2}^{2} & \\sqrt{2}x_{1}x_{2} \\end{pmatrix}\n",
    "     \\begin{pmatrix} y_{1}^{2} \\\\ y_{2}^{2} \\\\ \\sqrt{2}y_{1}y_{2} \\end{pmatrix} \\\\\n",
    "  &= x_{1}^{2} y_{1}^{2} + x_{2}^{2} y_{2}^{2} + 2 x_{1}x_{2} y_{1}y_{2} \\\\\n",
    "  &= (x_{1} y_{1} + x_{2} y_{2})^{2} \\\\\n",
    "  &= (x^{T}y)^{2}.\n",
    "\\end{align*}\n",
    "\n",
    "In this case the kernel is \n",
    "$$\n",
    "  K(x, y) = (x^{T}y)^{2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#191; $\\phi(x)$ or $K(x,y)$ ?\n",
    "\n",
    "If we work with the dual,\n",
    "$$\n",
    "  \\begin{array}{ll}\n",
    "    \\maximize_{\\mu_{1}, \\ldots, \\mu_{N}} & \\sum_{i=1}^{N} \\mu_{i} \n",
    "      - \\half \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\mu_{i}\\mu_{j}y_{i}y_{j} K(x^{(i)}, x^{(j)}) \\\\\n",
    "    \\subjectto & \\mu_{i} \\geq 0,\\ i = 1, \\ldots, N \\\\\n",
    "               & \\sum_{i=1}^{N} \\mu_{i}y_{i} = 0.\n",
    "  \\end{array}\n",
    "$$\n",
    "they we don't need to know $\\phi$ &ndash; only $K$.\n",
    "\n",
    "This leads to the following question: rather than choose a nonlinear transformation $\\phi$ to apply to our inputs, can we just choose the kernel $K$ associated with $\\phi$?\n",
    "\n",
    "So, given a function $K(x,y)$, when is there a function $\\phi$ such that \n",
    "$$\n",
    "  K(x,y) = \\phi(x)^{T} \\phi(y)?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of kernels\n",
    "\n",
    "Clearly, $K$ needs to be **symmetric**:\n",
    "$$\n",
    "  K(x,y) = \\phi(x)^{T}\\phi(y) = \\phi(y)^{T}\\phi(x) = K(y,x).\n",
    "$$\n",
    "\n",
    "There is a less obvious condition $K$ must satisfy.  Given $x_{1}, \\ldots, x_{n}$, consider the $n \\times n$ matrix whose $(i,j)$ entry is $K(x^{(i)},x^{(i)})$:\n",
    "$$\n",
    "  \\begin{pmatrix} \n",
    "    K(x_{1},x_{1}) & K(x_{1},x_{2}) & \\cdots & K(x_{1},x_{n}) \\\\\n",
    "    K(x_{2},x_{1}) & K(x_{1},x_{2}) & \\cdots & K(x_{2},x_{n}) \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "    K(x_{n},x_{1}) & K(x_{n},x_{2}) & \\cdots & K(x_{n},x_{n})\n",
    "  \\end{pmatrix}\n",
    "  = \\begin{pmatrix} \n",
    "    \\phi(x_{1})^{T}\\phi(x_{1}) & \\phi(x_{1})^{T}\\phi(x_{2}) & \\cdots & \\phi(x_{1})^{T}\\phi(x_{n}) \\\\\n",
    "    \\phi(x_{2})^{T}\\phi(x_{1}) & \\phi(x_{2})^{T}\\phi(x_{2}) & \\cdots & \\phi(x_{2})^{T}\\phi(x_{n}) \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "    \\phi(x_{n})^{T}\\phi(x_{1}) & \\phi(x_{n})^{T}\\phi(x_{2}) & \\cdots & \\phi(x_{n})^{T}\\phi(x_{n}) \\\\\n",
    "      \\end{pmatrix}\n",
    "   = \n",
    "    \\begin{pmatrix} \n",
    "      \\phi(x_{1})^{T} \\\\\n",
    "      \\phi(x_{2})^{T} \\\\\n",
    "      \\vdots \\\\ \n",
    "      \\phi(x_{n})^{T}\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix} \n",
    "      \\phi(x_{1}) & \\phi(x_{2}) & \\cdots & \\phi(x_{n}) \n",
    "    \\end{pmatrix} \n",
    "   \\equiv \\Phi^{T} \\Phi.\n",
    "$$\n",
    "For any vector $u$ we have\n",
    "$$\n",
    "  u^{T} \\Phi^{T} \\Phi u = (\\Phi u)^{T} (\\Phi u) = \\twonormsq{\\Phi u} \\geq 0.\n",
    "$$\n",
    "This means that if $K$ is a kernel, then the matrix $(K(x^{(i)},x^{(i)}))$ must be positive semidefinite for all choices of $x_{1}, \\ldots, x_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A necessary and sufficient condition for kernels\n",
    "\n",
    "It turns out that symmetry and positive semidefiniteness are also sufficient.\n",
    "\n",
    "**Theorem.**\n",
    "Suppose $K(x,y)$ is symmetric: $K(x,y) = K(y,x)$.  Then there exists $\\phi$ such that $K(x,y) = \\phi(x)^{T}\\phi(y)$ if and only if the matrix $(K(x^{(i)},x^{(i)}))$ is positive semidefinite for any collection of $x^{(i)}$.\n",
    "</div>\n",
    "\n",
    "A more easily checked condition is\n",
    "\n",
    "**Theorem [Mercer, 1909]**\n",
    "Suppose $K(x,y)$ is symmetric: $K(x,y) = K(y,x)$.  Then there exists $\\phi$ such that $K(x,y) = \\phi(x)^{T}\\phi(y)$ if and only if \n",
    "$$\n",
    "    \\int\\!\\!\\!\\int K(x,y) g(x) g(y)\\ dx\\ dy \\geq 0\n",
    "$$\n",
    "for all continuous functions $g$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to summarize, you can either choose a nonlinear transformation $\\mathbf{\\phi}$ and solve\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\minimize_{w,b,\\xi} &\\qquad \\half \\twonormsq{w} + C \\sum_{i} \\xi_{i} \\\\\n",
    "    \\subjectto &\\qquad y_{i} (w^{T}\\mathbf{\\phi}(x^{(i)}) + b) \\geq 1 - \\xi_{i}, \\quad 1 = 1, \\ldots, N \\\\\n",
    "               &\\qquad \\vphantom{\\half} \\xi_{i} \\geq 0, \\quad i = 1, \\ldots, N,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "or you can choose a kernel $K$ and solve\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\maximize_{\\mu_{1}, \\ldots, \\mu_{N}} &\\qquad \\sum_{i=1}^{N} \\mu_{i} \n",
    "      - \\half \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\mu_{i}\\mu_{j}y_{i}y_{j} K(x^{(i)}, x^{(j)}) \\\\\n",
    "    \\subjectto &\\qquad 0 \\leq \\mu_{i} \\leq C,\\ i = 1, \\ldots, N \\\\\n",
    "               &\\qquad \\sum_{i=1}^{N} \\mu_{i}y_{i} = 0.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The $\\nu$-SVC\n",
    "\n",
    "A variant of the SVC approach is the $\\nu$-SVC.\n",
    "\n",
    "The kernel formulation of the maximum-margin classifier is\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\minimize_{\\mu_{1}, \\ldots, \\mu_{N}} &\\qquad \\sum_{i=1}^{N} \\mu_{i} \n",
    "      - \\half \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\mu_{i}\\mu_{j}y_{i}y_{j} K(x^{(i)}, x^{(j)}) \\\\\n",
    "    \\subjectto &\\qquad 0 \\leq \\mu_{i} \\leq C,\\ i = 1, \\ldots, N \\\\\n",
    "               &\\qquad \\sum_{i=1}^{N} \\mu_{i}y_{i} = 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The $\\nu$-SVC approach eliminates the penalty weight $C$ with an alternative formulation of the problem:\n",
    "$$\n",
    "  \\begin{align*}\n",
    "    \\minimize_{\\mu_{1}, \\ldots, \\mu_{N}} &\\qquad\n",
    "      - \\half \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\mu_{i}\\mu_{j}y_{i}y_{j} K(x^{(i)}, x^{(j)}) \\\\\n",
    "    \\subjectto &\\qquad 0 \\leq \\mu_{i} \\leq 1/N,\\ i = 1, \\ldots, N \\\\\n",
    "               &\\qquad \\sum_{i=1}^{N} \\mu_{i}y_{i} = 0 \\\\\n",
    "               &\\qquad \\sum_{i=1}^{N} \\mu_{i} \\geq \\nu.\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "We choose the parameter $\\nu > 0$.  \n",
    "\n",
    "This approach is attractive because we no longer have to guess the \"right\" value for $C$, while $\\nu$ can be directly interpreted:\n",
    "1. $\\nu$ is a lower bound on the fraction of the training cases that will be support vectors;\n",
    "2. $\\nu$ is an upper bound on the fraction of margin errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The kernel classifier\n",
    "\n",
    "After choosing the kernel $K$ and solving the dual, it appears we still need the $\\phi$ in order to compute the support vectors:\n",
    "$$\n",
    "  w = \\sum_{i=1}^{N} \\mu_{i} y_{i} \\phi(x^{(i)}).\n",
    "$$\n",
    "\n",
    "However, the support vectors will only be used in the classifier, and we have\n",
    "$$\n",
    "  y(x) \n",
    "  = w^{T}\\mathbf{\\phi}(x) + b\n",
    "  = \\sum_{i=1}^{N} \\mu_{i} y_{i} \\phi(x^{(i)})^{T} \\phi(x) + b\n",
    "  = \\sum_{i=1}^{N} \\mu_{i} y_{i} K(x^{(i)}, x) + b.\n",
    "$$\n",
    "\n",
    "Thus, in order to build the classifier it suffices to know the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of kernels\n",
    "\n",
    "Here are several commonly used kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian or radial basis function (RBF) kernel\n",
    "\n",
    "$$\n",
    "  K(x,y) = \\exp(-\\gamma \\twonormsq{x-y})\n",
    "$$\n",
    "\n",
    "This is a very popular and frequently effective kernel.  The effectiveness likely derives from the fact that the feature space is infinite-dimensional (high-dimensional, in practice), giving us room to maneuver.  See the course notes for the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic kernel\n",
    "\n",
    "$$\n",
    "  K(x,y) = (x^{T}y)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial kernel\n",
    "\n",
    "$$\n",
    "  K(x,y) = (x^{T}y + c)^{m},\\ c > 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoidal kernel\n",
    "\n",
    "$$\n",
    "  K(x,y) = \\tanh (a x^{T}y + r),\\ a > 0, r < 0\n",
    "$$\n",
    "\n",
    "The sigmoidal kernel is not positive definite, but does yield a kernel function for all $r$ sufficiently negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new kernels from existing kernels <a id=\"new_kernels\"/>\n",
    "\n",
    "Given a kernel, we can create other kernels by a variety of transformations.  If $K_{1}(x,y)$ and $K_{2}(x,y)$ are kernels, then so are the following:\n",
    "\\begin{align*}\n",
    "  \\newcommand{\\kone}{K_{1}}\n",
    "  \\newcommand{\\ktwo}{K_{2}}\n",
    "  K(x,y) &= c \\kone(x,y),\\ \\mbox{if $c > 0$,} \\\\\n",
    "  K(x,y) &= f(x) \\kone(x,y) f(y),\\ \\mbox{if $f$ is real-valued,} \\\\\n",
    "  K(x,y) &= q(\\kone(x,y)),\\ \\mbox{if $q$ is a polynomial with nonegative coefficients,} \\\\\n",
    "  K(x,y) &= \\exp(\\kone(x,y)), \\\\\n",
    "  K(x,y) &= \\kone(x,y) + \\ktwo(x,y), \\\\\n",
    "  K(x,y) &= \\kone(x,y) \\ktwo(x,y).\n",
    "\\end{align*}\n",
    "\n",
    "If $\\mathbf{\\phi}(x) \\in \\R^{M}$ and $K_{3}$ is a kernel on $\\R^{M}$, then\n",
    "$$\n",
    "  K(x,y) = K_{3}(\\mathbf{\\phi}(x),\\mathbf{\\phi}(y))\n",
    "$$\n",
    "is also a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a kernel SVC in Scikit-Learn\n",
    "\n",
    "We will use the scikit-learn SVC (support vector classifier) module.  SVC is based on [libSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm), which is also used in the R package [e1071](https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf).\n",
    "\n",
    "The SciKit-Learn documentation is [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "We will use Fisher's iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "#classes = {'setosa':0, 'versicolor':1, 'virginica':2, 'none':0}\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y, test_size=0.3, random_state=0)  # Be sure to set the random seed so the results are reproducible!\n",
    "\n",
    "kernel = SVC(kernel=\"rbf\", C = 1000)  # The default kernel is RBF.\n",
    "kernel.fit(X_train, y_train)\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "clf = kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the options for the SVC constructor we can see the hyperparameter weight $C$ given the penalty term for margin errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the classifier\n",
    "\n",
    "First, the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Results for the test set.\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Accuracy, precision, recall, and f-score.\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# The confusion matrix for the training data.\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "class_names=(\"I. setosa\", \"I. versicolor\", \"I. virginica\")\n",
    "ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, display_labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The preceding shows the misclassification of the test data.  But how did the SVC do on the training data?\n",
    "\n",
    "Let's find out&hellip;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for the training set.\")\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "# Accuracy, precision, recall, and f-score.\n",
    "print(metrics.classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confusion matrix for the training data.\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "class_names=(\"I. setosa\", \"I. versicolor\", \"I. virginica\")\n",
    "ConfusionMatrixDisplay.from_estimator(clf, X_train, y_train, display_labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels for symbolic inputs\n",
    "\n",
    "Kernels can also be applied to inputs that are symbolic, e.g., sets, graphs, and strings and text.\n",
    "\n",
    "For instance, let $S$ is a finite set, and consider the collection of all subsets of $S$.  \n",
    "\n",
    "$\\newcommand{\\abs}[1]{|\\; #1 \\;|}$\n",
    "If $X$ and $Y$ are subsets of $S$, and $\\abs{X \\cap Y}$ is the cardinality of their intersection, then the **intersection kernel** is \n",
    "$$\n",
    "  K(X,Y) = 2^{\\abs{X \\cap Y}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A string kernel\n",
    "\n",
    "Given two strings $X$ and $Y$ which are members of a set of strings $S$, let\n",
    "$$\n",
    "  K(X,Y) = \\mbox{the number of substrings in $X$ and $Y$ have in common}.\n",
    "$$\n",
    "\n",
    "If we list all possible substrings of elements of $S$, and define $\\phi(W)$ to be\n",
    "$$\n",
    "  \\phi_{i}(W) = \\left\\{\n",
    "    \\begin{array}{cl}\n",
    "      1 & \\mbox{if string $i$ is a substring of $W$} \\\\\n",
    "      0 & \\mbox{otherwise},\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "$$\n",
    "then \n",
    "$$\n",
    "  K(X,Y) = \\phi(X)^{T}\\phi(Y).\n",
    "$$\n",
    "\n",
    "In this case it is not practical to construct $\\phi$, since the number of possible substrings could be huge, but it is easy to work with the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The spectrum kernel\n",
    "\n",
    "The **spectrum kernel** was developed to build an SVC for protein classification,\n",
    "\n",
    "We will call the set of all length $k$ subsequences of a string its $k$-spectrum.\n",
    "\n",
    "Given $k$, and a string $x$ define the feature map\n",
    "$$\n",
    "  \\mathbf{\\phi}(x) = (\\phi_{a}(x))_{\\mbox{all subsequence $a$ from of length $k$ in our alphabet}},\n",
    "$$\n",
    "where \n",
    "$$\n",
    "  \\phi_{a}(x) = \\mbox{the number of times $a$ occurs in $x$}.\n",
    "$$\n",
    "The $k$-spectrum kernel is then\n",
    "$$\n",
    "  K(x,y) = \\mathbf{\\phi}(x)^{T}\\mathbf{\\phi}(y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook was brought to you by Savage Panda Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "IPython.display.YouTubeVideo('I-ovzUNno7g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
