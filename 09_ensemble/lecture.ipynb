{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "    <h1>CSCI 416-01/516-01, Fundamentals of AI/ML</h1>\n",
    "    <h1>Fall 2025</h1>\n",
    "    <h1>Ensemble learning</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "- [Ensemble learning](#Ensemble-learning)\n",
    "- [The wine data set](#The-wine-data-set)\n",
    "- [A majority vote classifier](#A-majority-vote-classifier)\n",
    "- [Bagging](#Bagging)\n",
    "- [Random forests](#Random-forests)\n",
    "- [Adaptive boosting of weak learners](#Adaptive-boosting-of-weak-learners)\n",
    "- [Feature selection using random forests](#Feature-selection-using-random-forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "<a href=\"http://www.despair.com\">\n",
    "  <img src=\"https://cdn.shopify.com/s/files/1/0535/6917/products/meetingsdemotivator_grande.jpeg\"/>\n",
    "</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "\n",
    "**Ensemble learning** refers to combining ML algorithms to create an algorithm that is better than any one of the constituent algorithms.\n",
    "\n",
    "Ensemble learning is particularly common for classifiers, so we will discuss ensemble learning for classifiers.\n",
    "\n",
    "We will look at the following approaches to ensemble learning:\n",
    "* majority vote classifiers,\n",
    "* bagging,\n",
    "* random forests,\n",
    "* boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The wine data set\n",
    "\n",
    "We will use the classic [wine data set](https://archive.ics.uci.edu/ml/datasets/wine).  The features are a variety of chemical properties of the wine; the labels are the quality of the wine as determined by \"experts\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "\n",
    "y = wine.target\n",
    "X = wine.data\n",
    "\n",
    "class_names = wine.target_names\n",
    "print(wine.feature_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "  train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disparity in magnitude between the features will likely give kNN trouble, so we will standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std  = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A majority vote classifier\n",
    "\n",
    "The idea here is simple:\n",
    "1. Build a collection of classifiers.\n",
    "2. Poll the classifiers and use the \"majority vote\" as the classification.\n",
    "\n",
    "We can either use the majority vote for the class label, or, if our classifiers produce probabilities for each class, we can take a weighted sum of the probabilities and take the largest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab a passel of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf4 = SVC(kernel='rbf', random_state=1, gamma=0.05)\n",
    "clf5 = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "for clf in [clf1, clf2, clf3, clf4, clf5]:\n",
    "  title = type(clf)\n",
    "  clf.fit(X_train_std, y_train)\n",
    "\n",
    "  # Use the classifier to make predictions for the test set\n",
    "  y_pred = clf.predict(X_test_std)\n",
    "\n",
    "  disp = ConfusionMatrixDisplay.from_estimator(clf, X_test_std, y_test, normalize=\"true\")\n",
    "  disp.ax_.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with results for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for clf in [clf1, clf2, clf3, clf4, clf5]:\n",
    "  title = type(clf)\n",
    "\n",
    "  # Use the classifier to make predictions for the training set.\n",
    "  y_pred = clf.predict(X_train_std)\n",
    "\n",
    "  disp = ConfusionMatrixDisplay.from_estimator(clf, X_train_std, y_train, normalize=\"true\")\n",
    "  disp.ax_.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate some majority vote ensembles\n",
    "\n",
    "We will use Scikit-Learn's [VotingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "est = [('lr', clf1), ('rf', clf2), \n",
    "       ('knn', clf3), ('svm', clf4), ('tree', clf5)]\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=est, voting='hard')\n",
    "eclf1 = eclf1.fit(X_train_std, y_train)\n",
    "#print(eclf1.predict(X_train))\n",
    "\n",
    "est = [('lr', clf1), ('rf', clf2), \n",
    "       ('knn', clf3), ('tree', clf5)]\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=est, voting='soft')\n",
    "eclf2 = eclf2.fit(X_train_std, y_train)\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=est, voting='soft', weights=[2,1,1,1])\n",
    "eclf3 = eclf3.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in [eclf1, eclf2, eclf3]:\n",
    "  # Use the classifier to make predictions for the test set.\n",
    "  title = type(clf)\n",
    "  y_pred = clf.predict(X_test_std)\n",
    "\n",
    "  disp = ConfusionMatrixDisplay.from_estimator(clf, X_test_std, y_test, normalize=\"true\")\n",
    "  disp.ax_.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with the results for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in [eclf1, eclf2, eclf3]:\n",
    "  # Use the classifier to make predictions for the training set.\n",
    "  title = type(clf)\n",
    "  y_pred = clf.predict(X_train_std)\n",
    "\n",
    "  disp = ConfusionMatrixDisplay.from_estimator(clf, X_train_std, y_train, normalize=\"true\")\n",
    "  disp.ax_.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Majority voting models are usually built from related models.  **Bagging**, which is a portmanteau word meaning **bootstrap aggregating**, is one method for producing related models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping in statistics\n",
    "\n",
    "In statistics, a bootstrap technique is one that is based on repeated sampling with replacement.\n",
    "\n",
    "Suppose we want to estimate the percentage of trees in the US that are white oaks.\n",
    "\n",
    "It is not feasible to survey every individual tree in the US (the *population*), so we rely on a sample of trees.  This leads to a fundamental problem in statistics: how to make inferences about the total population from a sample.\n",
    "\n",
    "The elementary approach is to look at the percentage of white oaks in our sample and use that as an estimate of the population.  This gives us a *single* estimate of the population percentage.\n",
    "\n",
    "The bootstrapping idea is to model inference about a population from a sample by repeatedly resampling the sample and performing inference about the sample from the resampled data.\n",
    "\n",
    "In our example, suppose we have a sample of $n$ trees.  In the resampling, we would draw $n$ times from our sample uniformly with replacement.  This means that most likely some trees will be repeated and some left out in any given resample.  The probability a tree is left out of a resample is\n",
    "$$\n",
    "(1 - \\frac{1}{n})^{n} \\approx e^{-1} = 0.368\n",
    "$$\n",
    "if $n$ is large.\n",
    "\n",
    "This resampling process is repeated a large number of times, say, 1,000 to 10,000 times.  For each of these bootstrap samples we compute the percentage of white oaks. From these percentages we can build a histogram of the bootstrap percentages. This provides an estimate of the shape of the distribution of the white oak percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an ensemble of classifiers from bootstrap samples\n",
    "\n",
    "Let \n",
    "* $M$ be the number of models we wish to build for our ensemble, and\n",
    "* $T$ be the training set,\n",
    "* $n$ be the number of elements in $T$.\n",
    "\n",
    "Then,\n",
    "\n",
    "For $m = 1, \\ldots, M$:\n",
    "* Build a bootstrap model $T_{m}$ by drawing $n$ training cases from $T$ with replacement.\n",
    "* Use $T_{m}$ to build a classifier $C_{m}$.\n",
    "\n",
    "The predictions of the $C_{m}$ are then combined to make a prediction by the ensemble algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests\n",
    "\n",
    "In **random forests** we combine decision trees via bagging.\n",
    "\n",
    "When decision trees are combined using bagging, the result is called a **random forest**.\n",
    "\n",
    "In building random forests, bagging is often combined with another idea: build each tree from a different randomly chosen subset of the features.  This is called **subspace sampling** or **random subspaces**.\n",
    "\n",
    "We will use the scikit-learn [BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) and [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "We use subspace sampling in which each tree in the ensemble uses only a single feature (<code>max_features=1</code>).\n",
    "\n",
    "We also use **stumps** &ndash; trees of height 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=1)\n",
    "\n",
    "# Bag our own classifier.\n",
    "bag = BaggingClassifier(n_estimators=500, bootstrap=True, bootstrap_features=False, n_jobs=1, random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500, n_jobs=-1, max_features=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Single tree')\n",
    "tree = tree.fit(X_train, y_train)\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred  = tree.predict(X_test)\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "tree_test  = accuracy_score(y_test,  y_test_pred)\n",
    "print(f\"Single tree train/test accuracies:   {tree_train:.3f}/{tree_test:.3f}\")\n",
    "cm = confusion_matrix(y_test, y_test_pred, normalize=\"true\")\n",
    "print('Normalized confusion matrix')\n",
    "print(cm)\n",
    "print(72*'=')\n",
    "\n",
    "print('A bag of trees')\n",
    "bag = bag.fit(X_train, y_train)\n",
    "y_train_pred = bag.predict(X_train)\n",
    "y_test_pred  = bag.predict(X_test)\n",
    "bag_train = accuracy_score(y_train, y_train_pred) \n",
    "bag_test  = accuracy_score(y_test,  y_test_pred) \n",
    "print(f\"Bag of trees train/test accuracies:  {bag_train:.3f}/{bag_test:.3f}\")\n",
    "cm = confusion_matrix(y_test, y_test_pred, normalize=\"true\")\n",
    "print(\"Normalized confusion matrix\")\n",
    "print(cm)\n",
    "print(72*'=')\n",
    "\n",
    "print('Random forest')\n",
    "forest = forest.fit(X_train, y_train)\n",
    "y_train_pred = forest.predict(X_train)\n",
    "y_test_pred  = forest.predict(X_test)\n",
    "forest_train = accuracy_score(y_train, y_train_pred) \n",
    "forest_test  = accuracy_score(y_test,  y_test_pred) \n",
    "print(f\"Random forest train/test accuracies: {forest_train:.3f}/{forest_test:.3f}\")\n",
    "cm = confusion_matrix(y_test, y_test_pred, normalize=\"true\")\n",
    "print('Normalized confusion matrix')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive boosting of weak learners\n",
    "\n",
    "Some classifiers are called **weak learners**.\n",
    "A weak learner is a classifier that is not necessarily that good, but **is** better than random guessing, insofar as it consistently beats random guessing.\n",
    "\n",
    "We can apply **adaptive boosting** to weak learners to greatly improve their performance.\n",
    "\n",
    "In order for adapative boosting to work well on weak learners,\n",
    "1. we want to be able to train a weak learner quickly, since we are going to be building large numbers (hundreds to thousands) of them, and\n",
    "2. we also want a weak learner that makes predictions quickly.\n",
    "\n",
    "Decision trees make for good weak learners. By changing the maximum depth of the tree, we can control the training and prediction times, but also the potential for overfitting (variance).\n",
    "\n",
    "The classic weak learner is a stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The boosting idea\n",
    "\n",
    "Boosting is similiar to bagging, but it uses a more sophisticated technique than bootstrap resampling to create its training sets.\n",
    "\n",
    "Suppose we train a classifier and find that its training error rate is $\\varepsilon$.  We want to add another classifier to the ensemble that does better on the misclassifications of the first classifier.\n",
    "\n",
    "One way to do this is to duplicate the misclassified cases in our resampled training set.  This will shift the new classifier's attention to fixing the mistakes of the previous classifier.\n",
    "\n",
    "In practice, rather than duplicate misclassified cases, which makes the training set grow, we give them a higher weight.  For instance, in an SVM, we could add a weight term to the margin error for the misclassified cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weighting scheme in boosting\n",
    "\n",
    "How much should the weights change?\n",
    "\n",
    "The basic idea is that half of the total weight is assigned to the misclassified cases, and the other half to the rest.\n",
    "\n",
    "We start with uniform weights that sum to 1.  The current weight assigned to the misclassified examples is the error rate $\\varepsilon$, so we multiply their weights by $1/2\\varepsilon$:\n",
    "$$\n",
    "\\varepsilon \\times \\frac{1}{2\\varepsilon} = \\frac{1}{2}.\n",
    "$$\n",
    "Assuming $\\varepsilon < 0.5$ this increases the weight, as desired.\n",
    "\n",
    "The weights of the correctly classified examples are multiplied by $1/2(1-\\varepsilon)$.\n",
    "\n",
    "In the next round we do the same, except we take the non-uniform weights into account when evaluating the error rate.\n",
    "\n",
    "Suppose we have the classifier results\n",
    "<pre>\n",
    "              predicted pets    predicted food  total\n",
    "actual pets   24                16               40\n",
    "actual food    9                51               60\n",
    "total         33                67              100\n",
    "</pre>\n",
    "The error rate is $\\varepsilon = (9+16)/100 = 0.25$.  The weight update for the misclassified cases is $1/2\\varepsilon = 2$ and for the correctly classified cases $1/2(1-\\varepsilon) = 2/3$. \n",
    "\n",
    "Using these weights leads to the reweighted confusion matrix\n",
    "<pre>\n",
    "              predicted pets    predicted food  total\n",
    "actual pets   16                32               48\n",
    "actual food   18                34               60\n",
    "total         34                66              100\n",
    "</pre>\n",
    "Upon reweighting the error rate is 0.5.\n",
    "\n",
    "The last piece of the boosting algorithm is a confidence factor $\\alpha$ for each model in the ensemble.  This is used to compute the ensemble prediction, which is a weighted average of each individual model.\n",
    "\n",
    "We want $\\alpha$ to increase as $\\varepsilon$ decreases.  A common choice is \n",
    "$$\n",
    "\\alpha = \\frac{1}{2} \\ln\\frac{1-\\varepsilon}{\\varepsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=42)\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=5000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Single stump\")\n",
    "tree = tree.fit(X_train, y_train)\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred  = tree.predict(X_test)\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "tree_test  = accuracy_score(y_test,  y_test_pred)\n",
    "print(f\"Decision tree train/test accuracies {tree_train:.3f}/{tree_test:.3f}\")\n",
    "cm = confusion_matrix(y_test, y_test_pred, normalize=\"true\")\n",
    "print(\"Normalized confusion matrix\")\n",
    "print(cm)\n",
    "print(72*'=')\n",
    "\n",
    "print(\"Boosted stumps\")\n",
    "ada = ada.fit(X_train, y_train)\n",
    "y_train_pred = ada.predict(X_train)\n",
    "y_test_pred  = ada.predict(X_test)\n",
    "ada_train = accuracy_score(y_train, y_train_pred) \n",
    "ada_test  = accuracy_score(y_test,  y_test_pred) \n",
    "print(f\"AdaBoost train/test accuracies      {ada_train:.3f}/{ada_test:.3f}\")\n",
    "cm = confusion_matrix(y_test, y_test_pred, normalize=\"true\")\n",
    "print(\"Normalized confusion matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection using random forests\n",
    "\n",
    "**Feature selection** refers to the selection of the \"best\" or \"most useful\" subset of the raw features for use in an ML algorithm.\n",
    "\n",
    "**Feature extraction** refers to deriving new features from the raw features for use in an ML algorithm.\n",
    "\n",
    "Here we discuss feature selection using random forests.  Random forests can estimate the importance of a feature for classification by examining how much the decision tree nodes that use the feature reduce impurity.\n",
    "\n",
    "In estimating feature importance we average across all the nodes in all the trees in the random forest.\n",
    "\n",
    "In Scikit-Learn, this score is computed automatically for each feature, with the results being scaled to add to 1 when summed over all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=1)\n",
    "forest = forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random forest\")\n",
    "forest = forest.fit(X_train, y_train)\n",
    "y_train_pred = forest.predict(X_train)\n",
    "y_test_pred  = forest.predict(X_test)\n",
    "\n",
    "forest_train = accuracy_score(y_train, y_train_pred) \n",
    "forest_test  = accuracy_score(y_test,  y_test_pred) \n",
    "print(f\"Random forest train/test accuracies:       {forest_train:.3f}/{forest_test:.3f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred, normalize=\"true\")\n",
    "print('Normalized confusion matrix')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, score in zip(wine.feature_names, forest.feature_importances_):\n",
    "  print(f\"{name:32s} {score:f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A reduced model\n",
    "\n",
    "Let's build a model using only a subset of the original features.\n",
    "\n",
    "We will only keep features with importances of 0.06 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [0,6,9,10,11,12]\n",
    "\n",
    "X = X[:,columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "  train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random forest with reduced number of features')\n",
    "forest = forest.fit(X_train, y_train)\n",
    "y_train_pred = forest.predict(X_train)\n",
    "y_test_pred  = forest.predict(X_test)\n",
    "\n",
    "forest_train = accuracy_score(y_train, y_train_pred) \n",
    "forest_test  = accuracy_score(y_test,  y_test_pred) \n",
    "print(f\"Random forest train/test accuracies: {forest_train:.3f}/{forest_test:.3f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred, normalize=\"true\")\n",
    "print('Normalized confusion matrix')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a simpler model that does just as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is brought to you by tolerance.\n",
    "\n",
    "<blockquote>\n",
    "What is tolerance? It is the consequence of humanity.  We are all formed of frailty and error;\n",
    "let us pardon reciprocally one another’s folly - that is the first law of nature. <br/><br/>\n",
    "\n",
    "Qu’est-ce que la tolérance? c’est l’apanage de l’humanité.  Nous sommes tous pétris de faiblesses et d’erreurs;\n",
    "pardonnons-nous réciproquement nos sottises, c’est la première loi de la nature.<br/>\n",
    "&ndash; Voltaire (1694-1778), Dictionnaire philosophique, “Tolérance” (1764)\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
