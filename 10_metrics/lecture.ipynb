{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "    <h1>CSCI 416-01/516-01: Fundamentals of AI/ML</h1>\n",
    "    <h1>Fall 2025</h1>\n",
    "    <h1>Model evaluation and tuning</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wisconsin cancer dataset\n",
    "\n",
    "We will use the Wisconsin cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "print(data.DESCR)\n",
    "print(\"Feature names: \", data.feature_names)\n",
    "print(\"Target names:  \", data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target\n",
    "print(X[0:9,:])\n",
    "print(y[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version of the dataset in Scikit-Learn does not explain this, but the label 0 means **malignant** and label 1 means **benign**.  I find this confusing, so let's reverse the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (y == 0)\n",
    "\n",
    "print(y[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test sets\n",
    "\n",
    "We will use a 70/30 training/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining transformers and estimators in a pipeline\n",
    "\n",
    "Our pipeline will consist of (in order):\n",
    "1. the standard scaler (mean 0, variance 1), and\n",
    "3. a logistic regression classifier.\n",
    "\n",
    "We will also fit and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "            ('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print(f\"Test Accuracy: {pipe_lr.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using k-fold cross-validation to assess model performance\n",
    "\n",
    "[**Cross-validation**](http://scikit-learn.org/stable/modules/cross_validation.html) is one approach to assessing model performance.  Cross-validation allows us to assess our model **before** we try it on the test set.\n",
    "\n",
    "In basic k-fold cross-validation, we randomly partition the t training cases into k disjoint sets (**folds**) of size t/k.\n",
    "\n",
    "We then treat, in turn, each of the k sets as a validation set.\n",
    "\n",
    "For very small datasets one would typically choose t-fold cross-validation.  That is, there are t validation sets, each of size 1.  This is called **leave-out-one (LOO) validation**.\n",
    "\n",
    "**Stratified k-fold cross-validation** is a variation of k-fold cross-validation that can yield better results, particulary when the class proportions are markedly unequal.  In stratified cross-validation, the class proportions are preserved in each fold.  This ensures that each fold is representative of the class proportions in the training dataset.\n",
    "\n",
    "The term \"stratified\" comes from **stratified sampling**.  In stratified sampling, each sub-population, or stratum, is sampled independently so that the sample proportions will reflect the population proportions.  Stratification refers to the process of dividing the population into disjoint, homogeneous subgroups before sampling.\n",
    "\n",
    "We will use stratified cross-validation as implemented in the [StratifiedKFold class](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) to evaluate our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('no. of benign cases:   ', np.sum(y_train == 0))\n",
    "print('no. of malignant cases:', np.sum(y_train == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(skfold.split(X_train, y_train)):\n",
    "    pipe_lr.fit(X_train[train], y_train[train])\n",
    "    score = pipe_lr.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    print(f\"Fold: {k+1:2d}, Class distributions: {str(np.bincount(y_train[train])):s}, Accuracy: {score:f}\")\n",
    "    \n",
    "print()\n",
    "print(f\"Cross-validation accuracy: {np.mean(scores):.3f} +/- {np.std(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the same cross-validation score more succinctly using scikit-learn's [cross_val_score()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=pipe_lr, \n",
    "                         X=X_train, \n",
    "                         y=y_train, \n",
    "                         cv=10,\n",
    "                         n_jobs=1)\n",
    "print(f\"CV accuracy scores: {str(scores):s}\")\n",
    "print(f\"CV accuracy: {np.mean(scores):.3f} +/- {np.std(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is \"embarassingly parallel\" and the calculations for each fold can be spread across multiple processors independently.  We can set the number of cores/processors to use with the <code>n_jobs</code> argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging algorithms with learning curves\n",
    "\n",
    "An ML model must navigate the Scylla and Charybdis of **bias** and **variance**.\n",
    "\n",
    "Bias means we underfit the data due to modeling errors or other deficiencies in our model.  **Bias typically decreases with model complexity &ndash; more complex models are capable of capturing more features of the problem**.\n",
    "\n",
    "Variance refers to how our model's performance varies from one set of data to another.  **Variance increases with model complexity &ndash; in general, the more complex a model, the more sensitive it becomes to the choice of training data, and the more likely it is that the model will not generalize to new data because of overfitting to the training set**.\n",
    "\n",
    "There is a trade-off between bias and variance: a certain level of model complexity is needed to avoid bias, but too complex a model will suffer from high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing bias and variance problems with learning curves\n",
    "\n",
    "We can understand the bias and variance behavior of our model by examining how it does on training/test sets of varying size.\n",
    "* We look at the trends of performance on the training and test sets as the size of each increases.\n",
    "* When both sets are small, we should not expect great results.  But what happens as the size increases?\n",
    "* If the bias is large, we should not expect good performance for either the training set or the test set. &#128577; &#128577;\n",
    "* If the variance is high, we should see good performance on the training set but significantly poorer performance on the test set. &#128578; &#128577;\n",
    "\n",
    "Ideally we will have low bias and low variance achieving good accuracy on both the training and test sets. &#128578; &#128578;\n",
    "\n",
    "In Scikit-Learn we can generate plots like this using the <code>[learning_curve()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html)</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "            ('clf', LogisticRegression(penalty='l2', random_state=0))])\n",
    "\n",
    "train_sizes, train_scores, test_scores = \\\n",
    "                learning_curve(estimator=pipe_lr, \n",
    "                X=X_train, \n",
    "                y=y_train, \n",
    "                train_sizes=np.linspace(0.1, 1.0, 10), \n",
    "                cv=10,\n",
    "                n_jobs=1)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std  = np.std(train_scores, axis=1)\n",
    "test_mean  = np.mean(test_scores, axis=1)\n",
    "test_std   = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std, \n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes, \n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/learning_curve.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this classifier do with regards to bias (underfitting) and variance (overfitting)?\n",
    "\n",
    "Bias: &#128578;\n",
    "\n",
    "Variance: &#128578;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing overfitting and underfitting with validation curves\n",
    "\n",
    "We can often address overfitting (variance) and underfitting (bias) by tuning hyperparameters in our model during training.\n",
    "\n",
    "We can see the effects of changing one hyperparameter using Scikit-Learn's <code>[validation_curve()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html#sklearn.model_selection.validation_curve)</code> function.\n",
    "\n",
    "Let's look at logistic regression.  The problem being solved in [training the scikit-learn implementation of logistic regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) for binary classification is\n",
    "$$\n",
    "  \\newcommand{\\norm}[1]{\\| #1 \\|}\n",
    "  \\newcommand{\\minimize}{\\mbox{minimize}}\n",
    "  \\minimize_{w,b} \\sum_{i=1}^{N} \\log\\left(1 + \\exp(y_{i}(x_{i}^{T}w + b))\\right) + \\frac{1}{C} \\norm{w}_{2}^{2}.\n",
    "$$\n",
    "The term in the summation is the negative of the log-likelihood function (minimizing the negative is equivalent to maximizing the log-likelihood), with $y_{i} = \\pm 1$ being the class indicators.  We used this trick to simplify notation for SVC (compare the preceding with the formulation in the notes).\n",
    "\n",
    "$\\newcommand{\\twonorm}[1]{\\norm{#1}_{2}}$\n",
    "The term $\\frac{1}{C} \\twonorm{w}$ is a **regularization** or **penalty** terms that keep the model parameters from becoming too big.\n",
    "\n",
    "The weight $1/C$ controls the tradeoff between keeping $w$ small (a simpler model with more bias and less variance) and minimizing the negative log-likelihood (a more complex model with less bias and more variance).\n",
    "\n",
    "If $C$ is small, we are placing a greater emphasis on reducing variance; if $C$ is large we are are placing a greater emphasis on reducing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "train_scores, test_scores = validation_curve(\n",
    "                estimator=pipe_lr, \n",
    "                X=X_train, \n",
    "                y=y_train, \n",
    "                param_name='clf__C', \n",
    "                param_range=param_values,\n",
    "                cv=10)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_values, train_mean, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(param_values, train_mean + train_std,\n",
    "                 train_mean - train_std, alpha=0.15,\n",
    "                 color='blue')\n",
    "\n",
    "plt.plot(param_values, test_mean, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='validation accuracy')\n",
    "\n",
    "plt.fill_between(param_values,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When $C$ is small, the accuracies for the training and validation sets are similar, but the accuracy is lower than we can achieve for other values of $C$.  This is a sign of higher bias (underfitting), but lower variance (overfitting).\n",
    "* As $C$ increases, we obtain higher accuracy (lower bias) but performance on the two sets begins to diverge.  This is a sign of decreasing bias, but increasing variance (overfitting).\n",
    "* When $C$ is large, we obtain higher accuracy on the training set but decreasing accuracy on the validation set.  This is a sign of increasing variance and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters via grid search\n",
    "\n",
    "We can tune the hyperparameter values for our ML algorithms by searching over a grid of candidate values.\n",
    "\n",
    "The search is embarassingly parallel, so we can distribute it over multiple cores/processors.\n",
    "\n",
    "On the other hand, the number of models we need to train can grow quickly: if we are changing, say, 6 hyperparameters, and we want to try 10 values for each, then we have a $10 \\times 10 \\times 10 \\times 10 \\times 10 \\times 10$ grid, which has $10^{6}$ grid points! \n",
    "\n",
    "We can perform exhaustive grid search using scikit-learn's [<code>GridSearchCV()</code>](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) function.  Rather than train on a single model for each value of the hyperparameters, <code>GridSearchCV()</code> evaluates each model using cross-validation.  By default it uses three-fold stratified cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()),\n",
    "                     ('clf', SVC(random_state=1))])\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "param_grid = [{'clf__C': param_range, \n",
    "               'clf__kernel': ['linear']},\n",
    "                 {'clf__C': param_range, \n",
    "                  'clf__gamma': param_range, \n",
    "                  'clf__kernel': ['rbf']}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_svc, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  cv=10,\n",
    "                  n_jobs=-1)  # n_jobs = -1 means use all cores\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(f\"{gs.best_score_:.3f}\")\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Test accuracy: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm selection with nested cross-validation\n",
    "\n",
    "Cuidado! the use of cross-validation further increases the computation cost!\n",
    "\n",
    "For instance, if we pass <code>cv=2</code> in our call to <code>GridSearchCV()</code> to specify two-fold CV in the preceding example, then we are actually performing a $5 \\times 2$ nested CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=pipe_svc, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  cv=2)\n",
    "\n",
    "scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)\n",
    "print(f\"CV accuracy: {np.mean(scores):.3f} +/- {np.std(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is grid search applied to a decision tree.  We vary the maximum depth of the tree in the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0), \n",
    "                            param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}], \n",
    "                            scoring='accuracy', \n",
    "                            cv=2)\n",
    "scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)\n",
    "print('CV accuracy: {0:.3f} +/- {1:.3f}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized search\n",
    "\n",
    "There is also a [<code>RandomizedSearchCV()</code>](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) function that performs a randomized search of a subset of the hyperparameter space as a means of reducing the computational cost of tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The confusion matrix for the cancer data\n",
    "\n",
    "Let's look at the confusion matrix for the cancer data, using our SVC pipeline for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "y_pred = pipe_svc.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_svc,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Malignant\", \"Benign\"],\n",
    "    normalize=\"true\"\n",
    ")\n",
    "disp.ax_.set_title(\"Normalized confusion matrix for the SVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the precision and recall of a classification model\n",
    "\n",
    "The cancer data provide a good context for reviewing the concepts of precision and recall.\n",
    "* We will denote the malignant class by $\\oplus$, meaning \"positive for cancer\".\n",
    "* We will denote the benign class by $\\ominus$, meaning \"negative for cancer\".\n",
    "\n",
    "**Precision (P)**: of all the $\\oplus$'s you found, how many of them were really $\\oplus$'s?  Good precision means that the cases we diagnose as cancer really are cancer.\n",
    "\n",
    "**Recall (R)**: of all the $\\oplus$'s that are really there, how many of them did you find?  Good recall means that we are not missing cases of cancer.\n",
    "\n",
    "The F1 score measures both precision and recall by taking their harmonic mean:\n",
    "$$\n",
    "  \\mbox{F1} = \\frac{1}{\\frac{1}{2}\\left(\\frac{1}{P} + \\frac{1}{R}\\right)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(f\"Precision: {precision_score(y_true=y_test, y_pred=y_pred):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_true=y_test, y_pred=y_pred):.3f}\")\n",
    "print(f\"F1:        {f1_score(y_true=y_test, y_pred=y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the grid search algorithms use the performance metric (scoring function) provided by the classifier.\n",
    "\n",
    "However, we can create our own scorer and use that to optimize hyperparameters in a grid search.\n",
    "\n",
    "Here is an example of using the F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "c_gamma_range = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "param_grid = [{'clf__C': c_gamma_range, \n",
    "               'clf__kernel': ['linear']},\n",
    "                 {'clf__C': c_gamma_range, \n",
    "                  'clf__gamma': c_gamma_range, \n",
    "                  'clf__kernel': ['rbf'],}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_svc, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring=scorer, \n",
    "                  cv=2,\n",
    "                  n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision vs recall: which is right for you?\n",
    "\n",
    "**Precision**: of all the $\\oplus$'s you found, how many of them were really $\\oplus$'s?  Good precision means that the cases we diagnose as cancer really are cancer.\n",
    "\n",
    "**Recall**: of all the $\\oplus$'s that are really there, how many of them did you find?  Good recall means that we are not missing cases of cancer.\n",
    "\n",
    "Sometimes we care more about precision, sometimes we care more about recall, and sometimes we care about both.\n",
    "\n",
    "Suppose our classifier is deciding whether websites are safe for children.  Let $\\oplus$ be acceptable sites.  Then we probably care more about precision (not letting bad sites past) than recall (we're willing to reject quite a few good sites).\n",
    "\n",
    "One the other hand, if our classifier is looking for a disease, and $\\oplus$ are people with the disease, then we'd probably care more about recall (finding all the people with the disease) than precision (since once we see the people our dragnet rounds up we can tell they are not sick)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The precision/recall trade-off\n",
    "\n",
    "It's not always easy to achieve high precision and high recall at the same time.  \n",
    "\n",
    "In fact, increasing precision reduces recall, and increasing recall reduces precision.\n",
    "\n",
    "To understand this, let's consider logistic regression as implemented in scikit-learn.\n",
    "\n",
    "In binary classification, logistic regression builds a score $S(x)$ that instance $x$ belongs to one class (let's call it $\\oplus$).  If $S(x)$ exceeds some threshold, the model says that $x$ is an $\\oplus$; otherwise, it says $x$ is an $\\ominus$.\n",
    "\n",
    "Scikit-Learn won't let us see the threshold, but it will tell us the values of the score $S(x)$ it is using.  We can get at them using the ```decision_function()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gs.predict(X_train)\n",
    "y_scores = gs.decision_function(X_train)\n",
    "print('    score        class prediction')\n",
    "print(np.column_stack((y_scores, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that negative scores are Class 0 (benign) and positive scores are Class 1 (malignant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall vs scoring threshold\n",
    "\n",
    "Let's look at the effect of changing the scoring threshold on precision and recall.\n",
    "\n",
    "\n",
    "We use the [```precision_recall_curve()```](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) function to create a plot of the precision and recall for different values of the threshold used by in our classifier to decide whether cells are malignant or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"right\", fontsize=16)\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.xlim([np.amin(thresholds), np.amax(thresholds)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:\n",
    "- **Precision**: of all the $\\oplus$'s you found, how many of them were really $\\oplus$'s?  Good precision means that the cases we diagnose as cancer really are cancer.\n",
    "- **Recall**: of all the $\\oplus$'s that are really there, how many of them did you find?  Good recall means that we are not missing cases of cancer.\n",
    "\n",
    "We can see that as we increase the decision threshold in our classifier, the precision increases because we are making the requirement to be classified as $\\oplus$ more stringent.\n",
    "\n",
    "At the same time, recall is decreasing because we are missing more and more $\\oplus$ cases.\n",
    "\n",
    "Thus, if someone says they want 99.9% precision in their classsifier, they should also ask about the associated level of recall!  One is not meaningful without the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The precision-recall curve\n",
    "\n",
    "Now we will plot the **precision-recall** curve that shows the trade-off between precision and recall.\n",
    "\n",
    "We have values\n",
    "\\begin{align*}\n",
    "  (\\mbox{threshold}_{1}, \\mbox{precision}_{1}) &\\quad (\\mbox{threshold}_{1}, \\mbox{recall}_{1}) \\\\\n",
    "  (\\mbox{threshold}_{2}, \\mbox{precision}_{2}) &\\quad (\\mbox{threshold}_{2}, \\mbox{recall}_{2}) \\\\\n",
    "  \\vdots &\\quad \\vdots \\\\\n",
    "  (\\mbox{threshold}_{r}, \\mbox{precision}_{r}) &\\quad (\\mbox{threshold}_{r}, \\mbox{recall}_{r})\n",
    "\\end{align*}\n",
    "\n",
    "In the precision-recall curve we plot precision vs recall:\n",
    "\\begin{align*}\n",
    "  (\\mbox{recall}_{1}, \\mbox{precision}_{1}) \\\\\n",
    "  (\\mbox{recall}_{2}, \\mbox{precision}_{2}) \\\\\n",
    "  \\vdots \\\\\n",
    "  (\\mbox{recall}_{r}, \\mbox{precision}_{r}).\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good precision-recall plot.  As our recall improves, we do not lose much precision until the upper limit of recall is reached.\n",
    "\n",
    "A classifier with more room for improvement would be one that does not hug the upper right corner of the enclosing square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A precision-recall curve for an ungood classifier\n",
    "\n",
    "Let's look at an example where the classifier does not do well.  \n",
    "\n",
    "We will use two features of the iris data set and a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract sepal length and width from columns 0 and 1.\n",
    "iris_X = iris.data[:,[0,1]]\n",
    "iris_y = iris.target\n",
    "print(iris.target_names)\n",
    "\n",
    "# Select I. versicolor and I. virginica.\n",
    "mask = (iris_y != 0)\n",
    "iris_X = iris_X[mask,:]\n",
    "iris_y = iris_y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "iris_y = le.fit_transform(iris_y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = \\\n",
    "        train_test_split(iris_X, iris_y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr.fit(iris_X_train, iris_y_train)\n",
    "print(f\"Test accuracy: {lr.score(iris_X_test, iris_y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "iris_y_scores = lr.decision_function(iris_X_train)\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(iris_y_train, iris_y_scores)\n",
    "\n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a good precision-recall curve.  There is a rapid loss of precision as recall increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a receiver operating characteristic curve\n",
    "\n",
    "Another tool for evaluating binary classifiers is the **receiver operating characteristic (ROC) curve** or **normalized coverage plot**.\n",
    "The name comes from ROC curves in radio engineering, which were introduced in World War II to measure radio and radar performance.\n",
    "\n",
    "Recall:\n",
    "* **true positives** are correctly classified positives;\n",
    "* **false positives** are incorrectly classified negatives.\n",
    "In true/false positive/negative, \n",
    "* positive/negative refers to the classifier's prediction, while \n",
    "* true/false refers to whether this prediction is correct.\n",
    " Let\n",
    "\\begin{align*}\n",
    "  P  &= \\mbox{true number of $\\oplus$'s}, \\\\\n",
    "  N  &= \\mbox{true number of $\\ominus$'s}, \\\\\n",
    "  TP &= \\mbox{number of true positives}, \\\\\n",
    "  TN &= \\mbox{number of true negatives}, \\\\\n",
    "  FP &= \\mbox{number of false positives}, \\\\\n",
    "  FN &= \\mbox{number of false negatives}.\n",
    "\\end{align*}\n",
    "\n",
    "The **true positive rate** (TPR) and **false positive rate** (FPR) are the fraction of positives correctly classified and negatives incorrectly classified, respectively:\n",
    "\\begin{align*}\n",
    "  TPR &= \\frac{TP}{P} = \\frac{TP}{FN + TP}, \\\\\n",
    "  FPR &= \\frac{FP}{N} = \\frac{FP}{FP + TN}.\n",
    "\\end{align*}\n",
    "\n",
    "An ROC curve is similar to that in the precision-recall curve.  For various levels of the scoring threshold we compute the associated true positive rate (TPR) and false positive rate (FPR):\n",
    "\\begin{align*}\n",
    "  (\\mbox{threshold}_{1}, \\mbox{TPR}_{1}) &\\quad (\\mbox{threshold}_{1}, \\mbox{FPR}_{1}) \\\\\n",
    "  (\\mbox{threshold}_{2}, \\mbox{TPR}_{2}) &\\quad (\\mbox{threshold}_{2}, \\mbox{FPR}_{2}) \\\\\n",
    "  \\vdots &\\quad \\vdots \\\\\n",
    "  (\\mbox{threshold}_{r}, \\mbox{TPR}_{r}) &\\quad (\\mbox{threshold}_{r}, \\mbox{FPR}_{r})\n",
    "\\end{align*}\n",
    "\n",
    "In an ROC curve we plot the true positive rates (vertical axis) versus the false negative rates (horizontal axis):\n",
    "\\begin{align*}\n",
    "  (\\mbox{FPR}_{1}, \\mbox{TPR}_{1}) \\\\\n",
    "  (\\mbox{FPR}_{2}, \\mbox{TPR}_{2}) \\\\\n",
    "  \\vdots \\\\\n",
    "  (\\mbox{FPR}_{r}, \\mbox{TPR}_{r}).\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue curve is the ROC curve for our classifier.\n",
    "\n",
    "The dotted 45&#176; line represents the performance of a random classifier.  By that we mean a classifier that selects a random number $S$ from a uniform distribution on the interval [0,1], and classifies an instance as $\\oplus$ if $S > \\theta$, where $\\theta$ is the scoring threshold.\n",
    "* At one extreme, when $\\theta = 0$, all instances are classified as $\\oplus$.  This means TPR = 1 and FPR = 1.\n",
    "* At the other extreme, when $\\theta = 1$, all instances are classified as $\\ominus$.  This means TPR = 0 and FPR = 0.\n",
    "* For intermediate values of $\\theta$, equal fractions of true $\\oplus$ and true $\\ominus$ are classified as $\\oplus$, so TPR = FPR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The area under the ROC curve\n",
    "\n",
    "From the perspective of the ROC curve, the ideal classifier would be the constant value 1.\n",
    "* Such a classifier would always have perfect precision.\n",
    "* The area under such a curve would be 1 (height = 1 $\\times$ width = 1).\n",
    "* The area under the random classifier's ROC is 1/2.\n",
    "\n",
    "Th\n",
    "e area under an ROC curve is abbreviated AUC: **area under curve**.  AUC values near 1 are desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "y_pred = gs.predict(X_train)\n",
    "\n",
    "print('ROC AUC:  {0:.3f}'.format(roc_auc_score(y_train, y_scores)))\n",
    "print('Accuracy: {0:.3f}'.format(accuracy_score(y_train, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The precision/recall curve vs the ROC curve\n",
    "\n",
    "The precision/recall curve typically gives better insight when the number of $\\oplus$ cases is small and you care more about false positives than false negatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
