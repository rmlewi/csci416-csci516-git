{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "    <h1>CSCI 416-01/516-01, Fundamentals of AI/ML</h1>\n",
    "    <h1>Fall 2025</h1>\n",
    "    <h1>An introduction to Keras via CNNs</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* [Keras](#Keras)\n",
    "* [Getting started](#Getting-started)\n",
    "* [Prepare the data](#Prepare-the-data)\n",
    "    * [Add a fake color channel](@Add-a-fake-color_channel])\n",
    "* Specify the CNN\n",
    "    * Examine the layers in the CNN\n",
    "* Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Keras\n",
    "\n",
    "Tensorflow is Google's low-level framework for neural networks.  Keras is a high-level interface to Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "There is a version of Keras in Tensorflow as well as a standalone Keras module.  The one in Tensorflow lags the standalone version by a few versions.\n",
    "\n",
    "Tensorflow is **big**.  Note how long it takes to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras  # We'll use the Keras in Tensorflow.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{tf.__version__ = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of randomness in these tools.  We set seeds for the pseudo-random number generators (PRNGs) so that there is some hope we will get the same results if we execute the notebook again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make the randomness repeatable.\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "We will use the Fashion MNIST image dataset.  This is an image dataset of 10 types of clothing (e.g., t-shirts, sandals, shoes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = keras.datasets.fashion_mnist\n",
    "(X_train_all, y_train_all), (X_test, y_test) = fmnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into a training set and a validation set.\n",
    "X_valid, X_train = X_train_all[:5000], X_train_all[5000:]\n",
    "y_valid, y_train = y_train_all[:5000], y_train_all[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a fake color channel\n",
    "\n",
    "Keras expects images to have one or more color channels.  The greyscale images do not have one, so we need to add one.  This changes the tensors to have rank 4.  For instance, X_train changes from (55000,28,28) to (55000,28,28,1).\n",
    "\n",
    "We also make an explicit conversion from numpy arrays to tensorflow tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.expand_dims(tf.convert_to_tensor(X_train), axis=-1)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "\n",
    "X_valid = tf.expand_dims(tf.convert_to_tensor(X_valid), axis=-1)\n",
    "y_valid = tf.convert_to_tensor(y_valid)\n",
    "\n",
    "X_test = tf.expand_dims(tf.convert_to_tensor(X_test), axis=-1)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
    "               'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag',\n",
    "               'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of classes.\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the CNN\n",
    "\n",
    "NNs do not like large input values &ndash; they can cause problems during training, so our model will begin with rescaling the data to the interval $[0,1]$.\n",
    "\n",
    "The layers of the CNN are as follows:\n",
    "1. the input layer,\n",
    "2. a rescaling layer to map the data to the interval $[0,1]$.\n",
    "3. a convolutional layer consisting of 32 filters, each 3 x 3,\n",
    "4. a convolutional layer consisting of 64 filters, each 3 x 3,\n",
    "5. a max pooling layer to downsample the output of the previous layer by a factor of 2 in each dimension,\n",
    "6. a dropout layer which randomly selects nodes to turn off for dropout regularization,\n",
    "7. a flattening layer to turn the 2d image into a 1d vector,\n",
    "8. a dense layer with all-to-all connections,\n",
    "9. a second dropout layer, and finally\n",
    "10. a standard layer with all-to-all connections that produces as output softmax estimates of the class probabilities.\n",
    "\n",
    "In addition, in the call to the call to the model's [<code>compile</code>](https://keras.io/models/sequential) method we must also specify the [optimization algorithm](https://keras.io/optimizers) to be used in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('constructing the model...', end='')\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.InputLayer(shape=(28,28,1)))\n",
    "model.add(keras.layers.Rescaling(scale=1.0/255.0))\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the layers in the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CNN\n",
    "\n",
    "With large training sets one frequently takes optimization steps based on an approximate direction of steepest descent computed using only a subset of the the data.  We refer to the size of the subset used as the **batch size**.\n",
    "\n",
    "An **epoch** is a pass through the entire data set in the process of training/optimization.\n",
    "\n",
    "In each epoch, the batches are chosen randomly, whence the name **stochastic gradient descent** (SGD).\n",
    "\n",
    "SGD allows us to try steps more quickly, and avoid the time and space requirements of processing the entire training set.  If the training data are reasonably uniform in their distribution, a subset of the data should give results similar to the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPUs.\n",
    "print(\"GPUs detected:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "if not tf.config.list_physical_devices(\"GPU\"):\n",
    "    print('\\a')\n",
    "    print(90*'*')\n",
    "    print(\"*** Â¡Cuidado, llamas! ðŸ¦™ðŸ¦™ðŸ¦™  Â¡GPU detected!  Â¡Training will be much slower on a CPU! ***\")\n",
    "    print(90*'*')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few epochs of training\n",
    "\n",
    "Training is done with the method [<code>fit</code>](https://keras.io/models/sequential) in the <code>Sequential</code> model class.\n",
    "\n",
    "We also will use the test data as validation data.  This is useful to detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=4,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 6), use_index=[i for i in range(1,len(history.history)+1)])\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()\n",
    "print(len(history.history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test loss:    ', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is the training loss higher than the testing loss?\n",
    "\n",
    "Sometimes the accuracy reported for the training set is **lower** than that reported for the test set, and the loss report for the training set is **higher** than that reported for the test set.  This seems backwards.\n",
    "\n",
    "Per the [Keras FAQ](https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss):\n",
    "<blockquote>\n",
    "<p>\n",
    "A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.\n",
    "</p>\n",
    "<p>\n",
    "Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n",
    "</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis=1))\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper for our neural network.  It returns the \n",
    "# estimated class (rather than probabilities of class membership)\n",
    "# and pretends to be a scikit-learn classifier.\n",
    "\n",
    "class Wrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self._estimator_type = 'classifier'\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test).argmax(axis=1)\n",
    "    \n",
    "clf = Wrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, display_labels=class_names, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More epochs \n",
    "\n",
    "Keras makes it easy to continue the training where we left off earlier.  It saves the information needed to continue training the neural network.\n",
    "\n",
    "When we specify <code>initial_epoch</code>, the value of <code>epochs</code> is the epoch at which training ends, rather than the number of epochs we train over.\n",
    "\n",
    "That is, if we set <code>initial_epoch=3</code> and <code>epochs=4</code>, then we will only optimize over a single epoch.\n",
    "\n",
    "This convention is useful if we want to keep track of how many epochs of training have been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=8,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          initial_epoch=4)\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test loss:    ', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras callbacks \n",
    "\n",
    "Keras has a number of [callback](https://keras.io/callbacks) functions that can be used to specify actions to occur during training.\n",
    "\n",
    "Callbacks include\n",
    "* <code>ModelCheckpoint</code>, which saves the model after every epoch;\n",
    "* <code>EarlyStopping</code>, which stops the training when a specified metric of quality has stopped improving;\n",
    "* <code>ReduceLROnPlateau</code>, which reduces the learning rate when a specified metric of quality has stopped improving; and\n",
    "* <code>TerminateOnNaN</code>, which terminates the training when a NaN is computed as the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the neural network\n",
    "\n",
    "We can look at the weights for layer <code>k</code> using <code>model.layers[k].get_weights()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Saving the model\n",
    "\n",
    "We can use the model's <code>save</code> method to [save the model](https://keras.io/api/models/model_saving_apis/model_saving_and_loading/).\n",
    "\n",
    "Saving the model saves the layout and the weights in the neural network, and also the optimizer's state so that you can continue training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saving the model...', end='')\n",
    "model.save('fashion_mnist.keras')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the saved model using <code>keras.models.load_model</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model2 = load_model('fashion_mnist.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Data augmentation\n",
    "\n",
    "**Data augmentation** refers to creating new training examples from our training data.  This is especially useful when dealing with image data.\n",
    "\n",
    "We can create new images by applying random horizontal and vertical shifts to existing images.  This is useful for the digit data since the digits might not be perfectly centered.\n",
    "\n",
    "Other possible transmogrifications include flipping the image horizontally or vertically, rotating the images, and applying various types of scaling.  These augmentations are not helpful for the digit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reconstruct the model and train it using data augmentation to increase the effective size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('reconstructing the model...', end='')\n",
    "\n",
    "model2 = keras.models.Sequential()\n",
    "model2.add(keras.layers.InputLayer(shape=(28,28,1)))\n",
    "model2.add(keras.layers.Rescaling(scale=1.0/255.0))\n",
    "model2.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model2.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(keras.layers.Dropout(0.25))\n",
    "model2.add(keras.layers.Flatten())\n",
    "model2.add(keras.layers.Dense(128, activation='relu'))\n",
    "#model2.add(keras.layers.Dropout(0.5))\n",
    "model2.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model2.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to control the randomness.\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(54)\n",
    "\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "  width_shift_range=0.1,               # randomly shift images horizontally (fraction of total width)\n",
    "  height_shift_range=0.1,              # randomly shift images vertically (fraction of total height)\n",
    "                                       # The remaining options are here for illustrative purposes only.\n",
    "  featurewise_center=False,            # set input mean to 0 over the dataset\n",
    "  samplewise_center=False,             # set each sample mean to 0\n",
    "  featurewise_std_normalization=False, # divide inputs by std of the dataset\n",
    "  samplewise_std_normalization=False,  # divide each input by its std\n",
    "  zca_whitening=False,                 # apply ZCA whitening \n",
    "  rotation_range=0,                    # randomly rotate images in the range (in degrees, 0 to 180)\n",
    "  horizontal_flip=False,               # randomly flip images\n",
    "  vertical_flip=False)                 # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=128),\n",
    "                       epochs=4,\n",
    "                       verbose=1,\n",
    "                       validation_data=(X_valid, y_valid))\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test loss:    ', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
