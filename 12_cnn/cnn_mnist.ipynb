{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7b207d",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "    <h1>CSCI 416-01/516-01, Fundamentals of AI/ML</h1>\n",
    "    <h1>Fall 2025</h1>\n",
    "    <h1>A CNN for the MNIST digit dataset</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ae1ec",
   "metadata": {},
   "source": [
    "## Credit where credit is due\n",
    "\n",
    "Much of the code in the last section of this notebook was taken from notebooks that accompany the first edition of Chollet's *Deep Learning with Python*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca3c2a6-297f-4389-b22f-045ab398e34d",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6083b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0eb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make the randomness repeatable.\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(54)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f662c",
   "metadata": {},
   "source": [
    "# Read and preprocess the data\n",
    "\n",
    "We reshape the data to create a fake color channel.  We will later include a layer in our model to rescale the pixel values to the interval $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape((60000, 28, 28, 1))\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625a8c3",
   "metadata": {},
   "source": [
    "On a mad whim, let's encode the class labels using one-hot encoding.  Here are the original labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a30fa3",
   "metadata": {},
   "source": [
    "Now we transmogrify them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f250ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce3c07",
   "metadata": {},
   "source": [
    "For each label we have created a vector of length 10 with one entry for each class.  All entries are $0$ except for a $1$ in the component representing the class.\n",
    "\n",
    "For instance, the first image in the training set is a $5$, so there is a $1$ in component $5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4179958",
   "metadata": {},
   "source": [
    "# Build a CNN\n",
    "\n",
    "Our model starts with a rescaling layer.  Building preprocessing into the model ensures that it will be applied when the model is deployed.\n",
    "\n",
    "We then alternate convolutional layers with pooling layers; this is a common architecture.\n",
    "\n",
    "We end with a dense layer that is connected to a last dense layer that produces probability estimates for membership in each of the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf873ba8",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "\n",
    "**Pooling layers** downsample their input.  They do so by decomposing the input into blocks of specified size and keep only  part of the information about the block.  \n",
    "\n",
    "[Max pooling layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) only the maximum value for each block.\n",
    "\n",
    "[Average pooling layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D) retain the average of the values in the blocks.\n",
    "\n",
    "Keras also features [global max pooling layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool2D) and [global averaging pooling layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce5181",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "In our model each pooling layers operates on $2 \\times 2$ blocks.  Each pooling layer reduces the input to 25% of its original size.  At the end we flatten the data and pass it through two dense layers.  The last one produces probability estimates for membership in each of the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919291c-9cef-4c28-8c0d-54893f691ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb0bfe",
   "metadata": {},
   "source": [
    "While we specified an explicit input layer, it does not appear in the model summary.  Its role here is solely to make clear the shape of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04eb57e-c629-4e62-856a-503f1976c983",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58e6a5",
   "metadata": {},
   "source": [
    "Because we are using one-hot encoding of the class labels we specify ```categorical_crossentropy``` as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94caa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8ddc0",
   "metadata": {},
   "source": [
    "We will also specify some callbacks for the training phase.  We use [```EarlyStopping```](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) if 3 epochs (```patience=3```) elapse with no improvement in the validation accuracy (```monitor='val_accuracy'```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0864090",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048054d",
   "metadata": {},
   "source": [
    "We use [```ModelCheckpoint```](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the best model (as measured by validation accuracy) seen to date.  I am encountering a bug where without ```save_weights_only=True``` no file is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/tmp/checkpoints/mnist.keras'\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f4031",
   "metadata": {},
   "source": [
    "We specify that 10% of the training data should be set aside as the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2347e0f",
   "metadata": {},
   "source": [
    "<div class=\"danger\"></div>\n",
    "Keras chooses this set from <a target=\"_blank\" href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=de#fit\">the last samples before any random shuffling</a>.  Be sure your training data has been randomly shuffled before using <code>validation_split</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "              callbacks=[early_stopping, save_best],\n",
    "              epochs=5, \n",
    "              batch_size=128,\n",
    "              validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd18667",
   "metadata": {},
   "source": [
    "If we wish we can load the best model we saw in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27794a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a32e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74606e",
   "metadata": {},
   "source": [
    "It is always a good idea to examine the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(10, 6))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show() \n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbb625",
   "metadata": {},
   "source": [
    "The attribute ```history.history``` is simply a dictionary.  You can use this fact to splice together multiple histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29516b7",
   "metadata": {},
   "source": [
    "# Visualizing what a CNN does\n",
    "\n",
    "Keras makes it simple to look at the outputs of each layer of a NN.  Here we will look at the outputs of the convolutional and pooling layers, as they are 2-d objects.\n",
    "\n",
    "The following image is part of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image to plot.\n",
    "image = 560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_test[image])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca68ab42",
   "metadata": {},
   "source": [
    "Now we will build a Keras model that computes the outputs of the pairs of convolutional layers and pooling layers of our CNN.  These are all the layers up to the flattening layer that feeds into the dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97202b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = []\n",
    "for layer in model.layers[0:6]:\n",
    "    print(layer.name)\n",
    "    layer_names.append(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "# Extract the outputs of the pairs of convolutional and pooling layers.\n",
    "layer_outputs = [layer.output for layer in model.layers[0:6]]\n",
    "print(len(layer_outputs))\n",
    "\n",
    "# Create a model that will return these outputs, given the model input.\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "print(len(activation_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8eb23-6735-4286-87e3-1729f8ebf4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "layer_outputs = []\n",
    "layer_names = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n",
    "        layer_outputs.append(layer.output)\n",
    "        layer_names.append(layer.name)\n",
    "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6d37c",
   "metadata": {},
   "source": [
    "We use the new model's ```predict()``` method to run the test set through the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705870fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return a list of Numpy arrays, one array per\n",
    "# activation layer.\n",
    "activations = activation_model.predict(X_test)\n",
    "print(len(activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e41b6",
   "metadata": {},
   "source": [
    "Let's look at the output of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation = activations[1]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1f008",
   "metadata": {},
   "source": [
    "Here is the result of applying filter #3 to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(first_layer_activation[image, :, :, 4], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf0d4b",
   "metadata": {},
   "source": [
    "Now let's look at all of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_row = 16\n",
    "\n",
    "# Now let's display our feature maps.\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # This is the number of features in the feature map.\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features).\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # Tile the activation channels in this matrix.\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # Tile each filter into a big horizontal grid.\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[image,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Display the grid.\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef3da0",
   "metadata": {},
   "source": [
    "The first convolutional layer seems to contain horizontal and diagonal edge detectors.\n",
    "\n",
    "The second convolutional layer seems to further decompose the digit.\n",
    "\n",
    "However, after we exit the third convolutional layer it is not clear what we are looking at (at least not to me!).\n",
    "\n",
    "The blank tiles are interesting &ndash; these are filters that did not activate.  This means that the features those filters detect were not present in this particular image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf47d94-faf8-46a2-a1f5-ab7b373b98ab",
   "metadata": {},
   "source": [
    "# Softmax and cross-entropy\n",
    "\n",
    "Recall that probabilities lie in the range $0$ to $1$, and for a particular instance the sum over all the\n",
    "classes must be $1$.  How do we ensure our neural network outputs possess these properties?\n",
    "\n",
    "Suppose $(z_{1}, \\ldots, z_{m})$ are the outputs of the last hidden layer.   We can convert these into\n",
    "probability estimates using the **softmax function**.  The softmax function is defined via\n",
    "$$\n",
    "  y_{k} = \\frac{e^{z_{k}}}{\\sum_{j=1}^{m} e^{z_{j}}}.\n",
    "$$\n",
    "This function is smooth and behaves like a probability.  Clearly we have $0 < y_{k} < 1$ for all $k$, and if we\n",
    "sum over all the possible classes, we obtain $1$:\n",
    "$$\n",
    "  y_{1} + \\cdots + y_{m}\n",
    "  = \\sum_{k=1}^{m} \\frac{e^{z_{k}}}{\\sum_{j=1}^{m} e^{z_{j}}}\n",
    "  = \\frac{\\sum_{k=1}^{m} e^{z_{k}}}{\\sum_{j=1}^{m} e^{z_{j}}} = 1.\n",
    "$$\n",
    "\n",
    "Define\n",
    "$$\n",
    "  c_{ik} =\n",
    "  \\begin{cases}\n",
    "    1 & \\mbox{if training case $i$ belongs to class $k$,} \\\\\n",
    "    0 & \\mbox{otherwise.}\n",
    "  \\end{cases}\n",
    "$$\n",
    "Let $(y_{1}(x;W), \\ldots, y_{m}(x; W))$ be the softmax probability estimates the neural network computes\n",
    "given the input $x$ and model parameters $W$.  The likelihood associated with our training set is\n",
    "$$\n",
    "  \\prod_{x^{(i)} \\in {\\mathcal T}} \\prod_{k=1}^{m} y_{k}(x^{(i)}; W)^{c_{ik}}.\n",
    "$$\n",
    "Taking the logarithm we obtain the log-likelihood\n",
    "$$\n",
    "  \\sum_{x^{(i)} \\in {\\mathcal T}} \\sum_{k=1}^{m} c_{ik} \\log y_{k}(x^{(i)}; W).\n",
    "$$\n",
    "For each $i$, the quantity\n",
    "$$\n",
    "  - \\sum_{k=1}^{m} c_{ik} \\log y_{k}(x^{(i)}; W)\n",
    "$$\n",
    "is called the **cross-entropy** of $c$ and $y$.  We see that maximizing the log-likelihood for our neural networks is equivalent to minimizing the cross-entropy.\n",
    "\n",
    "Observe that since $0 < y_{k} < 1$ the cross-entropy is always positive.  We can ensure that the cross-entropy has a lower bound of zero by subtracting a constant:\n",
    "$$\n",
    "  - \\sum_{k=1}^{m} \\left( c_{ik} \\log y_{k}(x^{(i)}; W) - c_{ik} \\log c_{ik} \\right).\n",
    "$$\n",
    "This function is minimized when $y_{k}(x^{(i)}; W) = c_{kk}$ with minimum value 0, which makes it easier to monitor the progress of the optimization since we know where we would like ideally to end up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
